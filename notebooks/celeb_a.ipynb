{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ukb_age.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOGTUYyPnFEkj2yySc+VEao"},"kernelspec":{"name":"python387jvsc74a57bd0853417d72ad81a5e50a8613ae15c38dd8101027062a267a28f225259147f3710","display_name":"Python 3.8.7 64-bit"},"language_info":{"name":"python","version":"3.8.7"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import sys\n","module_path = os.path.abspath(os.path.join('..'))\n","if module_path not in sys.path:\n","    sys.path.append(module_path)"]},{"cell_type":"code","metadata":{"id":"_fpErrAwLmXJ","executionInfo":{"status":"ok","timestamp":1619898024903,"user_tz":-180,"elapsed":2686,"user":{"displayName":"Giora Simchoni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GidHkK_oHCDMfbBc7Imcn4FOEu_v_VTEcE_GqsdksU=s64","userId":"07886879145318559495"}}},"source":["import time\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import KFold\n","\n","from lmmnn.layers import NLL\n","from lmmnn.callbacks import EarlyStoppingWithSigmasConvergence, PrintSigmas\n","from lmmnn.menet import menet_fit_generator, menet_predict_generator\n","\n","import tensorflow as tf\n","import tensorflow.keras.backend as K\n","from tensorflow.keras import Model\n","from tensorflow.keras.callbacks import EarlyStopping, Callback\n","from tensorflow.keras.layers import (Concatenate, Conv2D, Dense, Dropout,\n","                                     Embedding, Flatten, Input, MaxPool2D,\n","                                     Reshape, GlobalAveragePooling2D, Layer)\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dBjo-wifNgQL","executionInfo":{"status":"ok","timestamp":1619898029438,"user_tz":-180,"elapsed":1464,"user":{"displayName":"Giora Simchoni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GidHkK_oHCDMfbBc7Imcn4FOEu_v_VTEcE_GqsdksU=s64","userId":"07886879145318559495"}}},"source":["gpus = tf.config.list_physical_devices('GPU')\n","for gpu in gpus:\n","    tf.config.experimental.set_memory_growth(gpu, True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RoiyOGU6Ot8N","executionInfo":{"status":"ok","timestamp":1619898073057,"user_tz":-180,"elapsed":596,"user":{"displayName":"Giora Simchoni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GidHkK_oHCDMfbBc7Imcn4FOEu_v_VTEcE_GqsdksU=s64","userId":"07886879145318559495"}}},"source":["class Count:\n","    curr = 0\n","\n","    def __init__(self, startWith=None):\n","        if startWith is not None:\n","            Count.curr = startWith - 1\n","\n","    def gen(self):\n","        while True:\n","            Count.curr += 1\n","            yield Count.curr\n","    \n","    def __call__(self):\n","        return Count.curr"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T-Hn9nKZN-tW","executionInfo":{"status":"ok","timestamp":1619970007568,"user_tz":-180,"elapsed":722,"user":{"displayName":"Giora Simchoni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GidHkK_oHCDMfbBc7Imcn4FOEu_v_VTEcE_GqsdksU=s64","userId":"07886879145318559495"}}},"source":["IMG_WIDTH = 178\n","IMG_HEIGHT = 218\n","batch_size = 20\n","epochs = 100\n","patience = 10\n","images_df = pd.read_csv('data/list_landmarks_align_celeba_processed.csv')\n","images_dir = 'data/img_align_celeba_png/'\n","images_df['nose_x'] = images_df['nose_x'].astype(np.float64)\n","images_df['celeb'] = images_df['celeb'] - 1\n","images_df.sort_values(by = ['celeb'], inplace=True)\n","res_df = pd.DataFrame(columns=['exp_type', 'mse', 'mae', 'sigma_e_est', 'sigma_b_est', 'n_epochs', 'time'])\n","counter = Count().gen()\n","n_cats = images_df['celeb'].max() + 1\n","kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","out_file = 'results/res_celeba.csv'\n","imgfile2celeb = images_df.set_index('img_file')['celeb'].to_dict()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A-a3a0BwPRON","executionInfo":{"status":"ok","timestamp":1619970026228,"user_tz":-180,"elapsed":1854,"user":{"displayName":"Giora Simchoni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GidHkK_oHCDMfbBc7Imcn4FOEu_v_VTEcE_GqsdksU=s64","userId":"07886879145318559495"}}},"source":["def sample_split(seed, train_index_subj, valid_frac = 0.1):\n","    np.random.seed(seed)\n","    train_n = len(train_index_subj)\n","    valid_samp = np.random.choice(train_n, int(valid_frac * train_n), replace=False)\n","    valid_index_subj = train_index_subj[valid_samp]\n","    train_index_subj = np.delete(train_index_subj, valid_samp)\n","    return train_index_subj, valid_index_subj\n","\n","def cnn_ignore():\n","    input_layer = Input((IMG_HEIGHT, IMG_WIDTH, 3))\n","    x = Conv2D(32, (5, 5), activation='relu')(input_layer)\n","    x = MaxPool2D((2, 2))(x)\n","    x = Conv2D(64, (5, 5), activation='relu')(x)\n","    x = MaxPool2D((2, 2))(x)\n","    x = Conv2D(32, (5, 5), activation='relu')(x)\n","    x = MaxPool2D((2, 2))(x)\n","    x = Conv2D(16, (5, 5), activation='relu')(x)\n","    x = MaxPool2D((2, 2))(x)\n","    x = Flatten()(x)\n","    x = Dropout(0.5)(x)\n","    x = Dense(100, activation='relu')(x)\n","    output = Dense(1)(x)\n","    return Model(inputs=[input_layer], outputs=output)\n","\n","def cnn_ignore_inception():\n","    base = InceptionV3(weights='imagenet', include_top=False, input_shape = (IMG_HEIGHT, IMG_WIDTH, 3))\n","    x = base.output\n","    x = GlobalAveragePooling2D()(x)\n","    output = Dense(1)(x)\n","    model = Model(inputs = base.input, outputs = output)\n","    train_top = 55\n","    for layer in model.layers[:-train_top]:\n","        layer.trainable = False\n","    for layer in model.layers[-train_top:]:\n","        layer.trainable = True\n","    return model\n","\n","def cnn_lmmnn():\n","    input_layer = Input((IMG_HEIGHT, IMG_WIDTH, 3))\n","    y_true_input = Input(shape=(1, ),)\n","    Z_input = Input(shape=(1, ), dtype=tf.int64)\n","    x = Conv2D(32, (5, 5), activation='relu')(input_layer)\n","    x = MaxPool2D((2, 2))(x)\n","    x = Conv2D(64, (5, 5), activation='relu')(x)\n","    x = MaxPool2D((2, 2))(x)\n","    x = Conv2D(32, (5, 5), activation='relu')(x)\n","    x = MaxPool2D((2, 2))(x)\n","    x = Conv2D(16, (5, 5), activation='relu')(x)\n","    x = MaxPool2D((2, 2))(x)\n","    x = Flatten()(x)\n","    x = Dropout(0.5)(x)\n","    x = Dense(100, activation='relu')(x)\n","    y_pred_output = Dense(1)(x)\n","    nll = NLL(10.0, 10.0)(y_true_input, y_pred_output, Z_input)\n","    return Model(inputs=[input_layer, y_true_input, Z_input], outputs=nll)\n","\n","def cnn_lmmnn_inception():\n","    y_true_input = Input(shape=(1, ),)\n","    Z_input = Input(shape=(1, ), dtype=tf.int64)\n","    base = InceptionV3(weights='imagenet', include_top=False, input_shape = (IMG_HEIGHT, IMG_WIDTH, 3))\n","    x = base.output\n","    x = GlobalAveragePooling2D()(x)\n","    y_pred_output = Dense(1)(x)\n","    nll = NLL(1.0, 1.0)(y_true_input, y_pred_output, Z_input)\n","    model = Model(inputs=[base.input, y_true_input, Z_input], outputs=nll)\n","    train_top = 58\n","    for layer in model.layers[:-train_top]:\n","        layer.trainable = False\n","    for layer in model.layers[-train_top:]:\n","        layer.trainable = True\n","    return model\n","\n","def cnn_embedding(n_cats, embed_dim):\n","    input_layer = Input((IMG_HEIGHT, IMG_WIDTH, 3))\n","    Z_input = Input(shape=(1,))\n","    embed = Embedding(n_cats, embed_dim, input_length = 1)(Z_input)\n","    embed = Reshape(target_shape = (embed_dim,))(embed)\n","    x = Conv2D(32, (5, 5), activation='relu')(input_layer)\n","    x = MaxPool2D((2, 2))(x)\n","    x = Conv2D(64, (5, 5), activation='relu')(x)\n","    x = MaxPool2D((2, 2))(x)\n","    x = Conv2D(32, (5, 5), activation='relu')(x)\n","    x = MaxPool2D((2, 2))(x)\n","    x = Conv2D(16, (5, 5), activation='relu')(x)\n","    x = MaxPool2D((2, 2))(x)\n","    x = Flatten()(x)\n","    x = Dropout(0.5)(x)\n","    x = Dense(100, activation='relu')(x)\n","    concat = Concatenate()([x, embed])\n","    output = Dense(1)(concat)\n","    return Model(inputs=[input_layer, Z_input], outputs=output)\n","\n","def cnn_embedding_inception(n_cats, embed_dim):\n","    base = InceptionV3(weights='imagenet', include_top=False, input_shape = (IMG_HEIGHT, IMG_WIDTH, 3))\n","    Z_input = Input(shape=(1,))\n","    embed = Embedding(n_cats, embed_dim, input_length = 1)(Z_input)\n","    embed = Reshape(target_shape = (embed_dim,))(embed)\n","    x = base.output\n","    x = GlobalAveragePooling2D()(x)\n","    concat = Concatenate()([x, embed])\n","    output = Dense(1)(concat)\n","    model = Model(inputs = [base.input, Z_input], outputs = output)\n","    train_top = 55\n","    for layer in model.layers[:-train_top]:\n","        layer.trainable = False\n","    for layer in model.layers[-train_top:]:\n","        layer.trainable = True\n","    return model\n","\n","def calc_b_hat(Z_train, y_train, y_pred_tr, n_cats, sig2e, sig2b):\n","    b_hat = []\n","    for i in range(n_cats):\n","        i_vec = Z_train == i\n","        n_i = i_vec.sum()\n","        if n_i > 0:\n","            y_bar_i = y_train[i_vec].mean()\n","            y_pred_i = y_pred_tr[i_vec].mean()\n","            # BP(b_i) = (n_i * sig2b / (sig2a + n_i * sig2b)) * (y_bar_i - y_pred_bar_i)\n","            b_i = n_i * sig2b * (y_bar_i - y_pred_i) / (sig2e + n_i * sig2b)\n","        else:\n","            b_i = 0\n","        b_hat.append(b_i)\n","    return np.array(b_hat)\n","\n","def custom_train_generator_lmmnn(train_generator, epochs):\n","    count = 0\n","    while True:\n","        if count == train_generator.n * epochs:\n","            train_generator.reset()\n","            break\n","        count += train_generator.batch_size\n","        data = train_generator.next()\n","        imgs = data[0]\n","        y_true = data[1][:, 0]\n","        Z = data[1][:, 1]\n","        yield [imgs, y_true, Z], None\n","\n","def custom_valid_generator_lmmnn(valid_generator, epochs):\n","    count = 0 \n","    while True:\n","        if count == valid_generator.n * epochs:\n","            valid_generator.reset()\n","            break\n","        count += valid_generator.batch_size\n","        data = valid_generator.next()\n","        imgs = data[0]\n","        y_true = data[1][:, 0]\n","        Z = data[1][:, 1]\n","        yield [imgs, y_true, Z], None\n","\n","def custom_test_generator_lmmnn(test_generator, epochs):\n","    count = 0 \n","    while True:\n","        if count == test_generator.n * epochs:\n","            test_generator.reset()\n","            break\n","        count += test_generator.batch_size\n","        data = test_generator.next()\n","        imgs = data[0]\n","        y_true = data[1][:, 0]\n","        Z = data[1][:, 1]\n","        yield [imgs, y_true, Z], None\n","\n","def custom_train_generator_embed(train_generator, epochs):\n","    count = 0 \n","    while True:\n","        if count == train_generator.n * epochs:\n","            train_generator.reset()\n","            break\n","        count += train_generator.batch_size\n","        data = train_generator.next()\n","        imgs = data[0]\n","        y_true = data[1][:, 0]\n","        Z = data[1][:, 1]\n","        yield [imgs, Z], y_true\n","\n","def custom_valid_generator_embed(valid_generator, epochs):\n","    count = 0 \n","    while True:\n","        if count == valid_generator.n * epochs:\n","            valid_generator.reset()\n","            break\n","        count += valid_generator.batch_size\n","        data = valid_generator.next()\n","        imgs = data[0]\n","        y_true = data[1][:, 0]\n","        Z = data[1][:, 1]\n","        yield [imgs, Z], y_true\n","\n","def custom_test_generator_embed(test_generator, epochs):\n","    count = 0 \n","    while True:\n","        if count == test_generator.n * epochs:\n","            test_generator.reset()\n","            break\n","        count += test_generator.batch_size\n","        data = test_generator.next()\n","        imgs = data[0]\n","        y_true = data[1][:, 0]\n","        Z = data[1][:, 1]\n","        yield [imgs, Z], y_true\n","\n","def factors(n):    # (cf. https://stackoverflow.com/a/15703327/849891)\n","    j = 2\n","    while n > 1:\n","        for i in range(j, int(np.sqrt(n+0.05)) + 1):\n","            if n % i == 0:\n","                n /= i ; j = i\n","                yield i\n","                break\n","        else:\n","            if n > 1:\n","                yield n; break\n","\n","def get_batchsize_steps(n):\n","    factors_n = list(factors(n))\n","    if len(factors_n) > 1:\n","        batch_size = factors_n[-2]\n","    else:\n","        batch_size = 1\n","    steps = n // batch_size\n","    return batch_size, steps\n","\n","def get_generators(images_df, images_dir, train_samp_subj, valid_samp_subj, test_samp_subj, batch_size, reg_type):\n","    train_datagen = ImageDataGenerator(rescale = 1./255) # preprocessing_function = preprocess_input # for inception\n","    valid_datagen = ImageDataGenerator(rescale = 1./255) # preprocessing_function = preprocess_input # for inception\n","    test_datagen = ImageDataGenerator(rescale = 1./255) # preprocessing_function = preprocess_input # for inception\n","    if reg_type in ['ignore', 'menet']:\n","        y_cols = ['age']\n","    else:\n","        y_cols = ['age', 'subject_id2']\n","    train_generator = train_datagen.flow_from_dataframe(\n","        images_df[images_df['subject_id2'].isin(train_samp_subj)],\n","        directory = images_dir,\n","        x_col = 'image_id',\n","        y_col = y_cols,\n","        target_size = (IMG_HEIGHT, IMG_WIDTH),\n","        class_mode = 'raw',\n","        batch_size = batch_size,\n","        shuffle = True,\n","        validate_filenames = False\n","    )\n","    valid_generator = valid_datagen.flow_from_dataframe(\n","        images_df[images_df['subject_id2'].isin(valid_samp_subj)],\n","        directory = images_dir,\n","        x_col = 'image_id',\n","        y_col = y_cols,\n","        target_size = (IMG_HEIGHT, IMG_WIDTH),\n","        class_mode = 'raw',\n","        batch_size = batch_size,\n","        shuffle = False,\n","        validate_filenames = False\n","    )\n","    test_generator = test_datagen.flow_from_dataframe(\n","        images_df[images_df['subject_id2'].isin(test_samp_subj)],\n","        directory = images_dir,\n","        x_col = 'image_id',\n","        y_col = y_cols,\n","        target_size = (IMG_HEIGHT, IMG_WIDTH),\n","        class_mode = 'raw',\n","        batch_size = batch_size,\n","        shuffle = False,\n","        validate_filenames = False\n","    )\n","    return train_generator, valid_generator, test_generator\n","\n","def reg_nn_ignore(train_generator, valid_generator, test_generator, n_cats, epochs, patience):\n","    model = cnn_ignore()\n","    model.compile(loss='mse', optimizer='adam')\n","    callbacks = [EarlyStopping(monitor='val_loss', patience=epochs if patience is None else patience)]\n","    history = model.fit(train_generator, validation_data = valid_generator, epochs=epochs, callbacks=callbacks, verbose=1)\n","    y_pred = model.predict(test_generator, verbose=1).reshape(test_generator.n)\n","    return y_pred, (None, None), len(history.history['loss'])\n","\n","def reg_nn_lmm(train_generator, valid_generator, test_generator, n_cats, epochs, patience):\n","    model = cnn_lmmnn()\n","    model.compile(optimizer= 'adam')\n","    \n","    patience = epochs if patience is None else patience\n","    callbacks = [EarlyStoppingWithSigmasConvergence(patience=patience), PrintSigmas()]\n","    step_size_train = train_generator.n // train_generator.batch_size\n","    step_size_valid = valid_generator.n // valid_generator.batch_size\n","    history = model.fit(custom_train_generator_lmmnn(train_generator, epochs), steps_per_epoch = step_size_train,\n","        validation_data = custom_valid_generator_lmmnn(valid_generator, epochs), validation_steps = step_size_valid,\n","        epochs=epochs, callbacks=callbacks, verbose=1)\n","    \n","    sig2e_est, sig2b_est = model.layers[-1].get_vars()\n","    \n","    batch_size_train, steps_train = get_batchsize_steps(train_generator.n)\n","    train_generator.reset()\n","    train_generator.batch_size = batch_size_train\n","    y_pred_tr = model.predict(custom_train_generator_lmmnn(train_generator, 1),\n","                              steps = steps_train,\n","                              verbose=1).reshape(train_generator.n)\n","    y_train = train_generator.labels[:, 0]\n","    Z_train = train_generator.labels[:, 1].astype(np.int)\n","    Z_test = test_generator.labels[:, 1].astype(np.int)\n","    b_hat = calc_b_hat(Z_train, y_train, y_pred_tr, n_cats, sig2e_est, sig2b_est)\n","    batch_size_test, steps_test = get_batchsize_steps(test_generator.n)\n","    test_generator.batch_size = batch_size_test\n","    y_pred = model.predict(custom_test_generator_lmmnn(test_generator, 1),\n","                           steps = steps_test, verbose=0).reshape(test_generator.n) + b_hat[Z_test]\n","    return y_pred, (sig2e_est, sig2b_est), len(history.history['loss'])\n","\n","def reg_nn_embed(train_generator, valid_generator, test_generator, n_cats, epochs, patience):\n","    embed_dim = 10\n","    model = cnn_embedding(n_cats, embed_dim)\n","    model.compile(loss='mse', optimizer='adam')\n","    callbacks = [EarlyStopping(monitor='val_loss', patience=epochs if patience is None else patience)]\n","    \n","    step_size_train = train_generator.n // train_generator.batch_size\n","    step_size_valid = valid_generator.n // valid_generator.batch_size\n","    \n","    history = model.fit(custom_train_generator_embed(train_generator, epochs), steps_per_epoch = step_size_train,\n","        validation_data = custom_valid_generator_embed(valid_generator, epochs),\n","        validation_steps = step_size_valid,\n","        epochs=epochs, callbacks=callbacks, verbose=1)\n","    batch_size_test, steps_test = get_batchsize_steps(test_generator.n)\n","    test_generator.batch_size = batch_size_test\n","    y_pred = model.predict(custom_test_generator_embed(test_generator, 1),\n","                           steps = steps_test, verbose=0).reshape(test_generator.n)\n","    return y_pred, (None, None), len(history.history['loss'])\n","\n","def reg_nn_menet(train_generator, valid_generator, test_generator, n_cats, epochs, patience):\n","    model = cnn_ignore()\n","    model.compile(loss='mse', optimizer='adam')\n","    callbacks = [EarlyStopping(monitor='val_loss', patience=epochs if patience is None else patience)]\n","\n","    clusters_train = np.array([imgfile2celeb[image_id] for image_id in train_generator.filenames])\n","    clusters_valid = np.array([imgfile2celeb[image_id] for image_id in valid_generator.filenames])\n","    clusters_test = np.array([imgfile2celeb[image_id] for image_id in test_generator.filenames])\n","\n","    model, b_hat, sig2e_est, n_epochs, _ = menet_fit_generator(model, train_generator, valid_generator,\n","        clusters_train, clusters_valid, n_cats,\n","        epochs=epochs, callbacks=callbacks, patience=patience, verbose=1)\n","    y_pred = menet_predict_generator(model, test_generator, clusters_test, n_cats, b_hat).reshape(test_generator.n)\n","    return y_pred, (sig2e_est, None), n_epochs\n","\n","def reg_nn(images_df, images_dir, train_samp_subj, valid_samp_subj, test_samp_subj,\n","    n_cats, batch_size=20, epochs=100, patience=10, reg_type='ignore'):\n","    start = time.time()\n","    train_generator, valid_generator, test_generator = get_generators(\n","        images_df, images_dir, train_samp_subj, valid_samp_subj, test_samp_subj, batch_size, reg_type)\n","    if reg_type == 'ignore':\n","        y_pred, sigmas, n_epochs = reg_nn_ignore(train_generator, valid_generator, test_generator, n_cats, epochs, patience)\n","    elif reg_type == 'lmm':\n","        y_pred, sigmas, n_epochs = reg_nn_lmm(train_generator, valid_generator, test_generator, n_cats, epochs, patience)\n","    elif reg_type == 'embed':\n","        y_pred, sigmas, n_epochs = reg_nn_embed(train_generator, valid_generator, test_generator, n_cats, epochs, patience)\n","    elif reg_type == 'menet':\n","        y_pred, sigmas, n_epochs = reg_nn_menet(train_generator, valid_generator, test_generator, n_cats, epochs, patience)\n","    else:\n","        raise ValueError(reg_type + ' is an unknown reg_type')\n","    end = time.time()\n","    y_test = test_generator.labels[:, 0]\n","    mse = np.mean((y_pred - y_test)**2)\n","    mae = np.mean(np.abs(y_pred - y_test))\n","    return mse, mae, sigmas, n_epochs, end - start\n","\n","def iterate_reg_types(images_df, images_dir, res_df, counter, n_cats, train_samp_subj, valid_samp_subj, test_samp_subj,\n","    out_file, batch_size, epochs, patience):\n","        mse_lmm, mae_lmm, sigmas, n_epochs_lmm, time_lmm = reg_nn(images_df, images_dir, train_samp_subj, valid_samp_subj,                          test_samp_subj, n_cats, batch_size=batch_size, epochs=epochs, patience=patience, reg_type='lmm')\n","        print(' finished lmm, mse: %.2f, mae: %.2f' % (mse_lmm, mae_lmm))\n","        mse_ig, mae_ig, _, n_epochs_ig, time_ig = reg_nn(images_df, images_dir, train_samp_subj, valid_samp_subj, test_samp_subj,                   n_cats, batch_size=batch_size, epochs=epochs, patience=patience, reg_type='ignore')\n","        print(' finished ignore, mse: %.2f, mae: %.2f' % (mse_ig, mae_ig))\n","        mse_em, mae_em, _, n_epochs_em, time_em = reg_nn(images_df, images_dir, train_samp_subj, valid_samp_subj, test_samp_subj,                   n_cats, batch_size=batch_size, epochs=epochs, patience=patience, reg_type='embed')\n","        print(' finished embed, mse: %.2f, mae: %.2f' % (mse_em, mae_em))\n","        mse_me, mae_me, sigmas_me, n_epochs_me, time_me = reg_nn(images_df, images_dir, train_samp_subj, valid_samp_subj,                           test_samp_subj, n_cats, batch_size=batch_size, epochs=epochs, patience=patience, reg_type='menet')\n","        print(' finished menet, mse: %.2f, mae: %.2f' % (mse_me, mae_me))\n","        res_df.loc[next(counter)] = ['ignore', mse_ig, mae_ig, np.nan, np.nan, n_epochs_ig, time_ig]\n","        res_df.loc[next(counter)] = ['lmm', mse_lmm, mae_lmm, sigmas[0], sigmas[1], n_epochs_lmm, time_lmm]\n","        res_df.loc[next(counter)] = ['embed', mse_em, mae_em, np.nan, np.nan, n_epochs_em, time_em]\n","        res_df.loc[next(counter)] = ['menet', mse_me, mae_me, sigmas_me[0], np.nan, n_epochs_me, time_me]\n","        res_df.to_csv(out_file)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NMUSQIdbO38e","outputId":"7edd1994-e239-43fe-eeaa-eabb4d8db756","tags":[]},"source":["for i, (train_index_subj, test_index_subj) in enumerate(kf.split(np.zeros(n_cats), np.zeros(n_cats))):\n","        print('iteration: %d' % i)\n","        train_index_subj, valid_index_subj = sample_split(i, train_index_subj)\n","        iterate_reg_types(images_df, images_dir, res_df, counter, n_cats, train_index_subj, valid_index_subj, test_index_subj,\n","            out_file, batch_size=batch_size, epochs=epochs, patience=patience)"],"execution_count":null,"outputs":[]}]}