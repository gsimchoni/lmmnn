{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ukb_age.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOGTUYyPnFEkj2yySc+VEao"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import sys\n","module_path = os.path.abspath(os.path.join('..'))\n","if module_path not in sys.path:\n","    sys.path.append(module_path)"]},{"cell_type":"code","metadata":{"id":"_fpErrAwLmXJ","executionInfo":{"status":"ok","timestamp":1619898024903,"user_tz":-180,"elapsed":2686,"user":{"displayName":"Giora Simchoni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GidHkK_oHCDMfbBc7Imcn4FOEu_v_VTEcE_GqsdksU=s64","userId":"07886879145318559495"}}},"source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import KFold\n","\n","from lmmnn.layers import NLL\n","from lmmnn.callbacks import EarlyStoppingWithSigmasConvergence, PrintSigmas\n","\n","import tensorflow as tf\n","import tensorflow.keras.backend as K\n","from tensorflow.keras import Model\n","from tensorflow.keras.callbacks import EarlyStopping, Callback\n","from tensorflow.keras.layers import (Concatenate, Conv2D, Dense, Dropout,\n","                                     Embedding, Flatten, Input, MaxPool2D,\n","                                     Reshape, GlobalAveragePooling2D, Layer)\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dBjo-wifNgQL","executionInfo":{"status":"ok","timestamp":1619898029438,"user_tz":-180,"elapsed":1464,"user":{"displayName":"Giora Simchoni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GidHkK_oHCDMfbBc7Imcn4FOEu_v_VTEcE_GqsdksU=s64","userId":"07886879145318559495"}}},"source":["gpus = tf.config.list_physical_devices('GPU')\n","for gpu in gpus:\n","    tf.config.experimental.set_memory_growth(gpu, True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RoiyOGU6Ot8N","executionInfo":{"status":"ok","timestamp":1619898073057,"user_tz":-180,"elapsed":596,"user":{"displayName":"Giora Simchoni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GidHkK_oHCDMfbBc7Imcn4FOEu_v_VTEcE_GqsdksU=s64","userId":"07886879145318559495"}}},"source":["class Count:\n","    curr = 0\n","\n","    def __init__(self, startWith=None):\n","        if startWith is not None:\n","            Count.curr = startWith - 1\n","\n","    def gen(self):\n","        while True:\n","            Count.curr += 1\n","            yield Count.curr\n","    \n","    def __call__(self):\n","        return Count.curr"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T-Hn9nKZN-tW","executionInfo":{"status":"ok","timestamp":1619970007568,"user_tz":-180,"elapsed":722,"user":{"displayName":"Giora Simchoni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GidHkK_oHCDMfbBc7Imcn4FOEu_v_VTEcE_GqsdksU=s64","userId":"07886879145318559495"}}},"source":["IMG_WIDTH = 299\n","batch_size = 20\n","epochs = 100\n","patience = 10\n","images_df = pd.read_csv('data/ukb_images_df.csv')\n","images_dir = 'data/ukb_images/'\n","images_df['age'] = images_df['age'].astype(np.float64)\n","res_df = pd.DataFrame(columns=['exp_type', 'mse', 'mae', 'sigma_e_est', 'sigma_b_est'])\n","counter = Count().gen()\n","n_cats = images_df['subject_id2'].max() + 1\n","kf = KFold(n_splits=5)\n","out_file = 'results/res_ukb_age.csv'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A-a3a0BwPRON","executionInfo":{"status":"ok","timestamp":1619970026228,"user_tz":-180,"elapsed":1854,"user":{"displayName":"Giora Simchoni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GidHkK_oHCDMfbBc7Imcn4FOEu_v_VTEcE_GqsdksU=s64","userId":"07886879145318559495"}}},"source":["def sample_split(seed, train_index_subj, valid_frac = 0.1):\n","    np.random.seed(seed)\n","    train_n = len(train_index_subj)\n","    valid_samp = np.random.choice(train_n, int(valid_frac * train_n), replace=False)\n","    valid_index_subj = train_index_subj[valid_samp]\n","    train_index_subj = np.delete(train_index_subj, valid_samp)\n","    return train_index_subj, valid_index_subj\n","\n","def cnn_ignore():\n","    input_layer = Input((IMG_WIDTH, IMG_WIDTH, 3))\n","    x = Conv2D(32, (5, 5), activation='relu')(input_layer)\n","    x = MaxPool2D((2, 2))(x)\n","    x = Conv2D(64, (5, 5), activation='relu')(x)\n","    x = MaxPool2D((2, 2))(x)\n","    x = Conv2D(32, (5, 5), activation='relu')(x)\n","    x = MaxPool2D((2, 2))(x)\n","    x = Conv2D(16, (5, 5), activation='relu')(x)\n","    x = MaxPool2D((2, 2))(x)\n","    x = Flatten()(x)\n","    x = Dropout(0.5)(x)\n","    x = Dense(100, activation='relu')(x)\n","    output = Dense(1)(x)\n","    return Model(inputs=[input_layer], outputs=output)\n","\n","def cnn_ignore_inception():\n","    base = InceptionV3(weights='imagenet', include_top=False, input_shape = (IMG_WIDTH, IMG_WIDTH, 3))\n","    x = base.output\n","    x = GlobalAveragePooling2D()(x)\n","    output = Dense(1)(x)\n","    model = Model(inputs = base.input, outputs = output)\n","    train_top = 55\n","    for layer in model.layers[:-train_top]:\n","        layer.trainable = False\n","    for layer in model.layers[-train_top:]:\n","        layer.trainable = True\n","    return model\n","\n","def cnn_lmmnn():\n","    input_layer = Input((IMG_WIDTH, IMG_WIDTH, 3))\n","    y_true_input = Input(shape=(1, ),)\n","    Z_input = Input(shape=(1, ), dtype=tf.int64)\n","    x = Conv2D(32, (5, 5), activation='relu')(input_layer)\n","    x = MaxPool2D((2, 2))(x)\n","    x = Conv2D(64, (5, 5), activation='relu')(x)\n","    x = MaxPool2D((2, 2))(x)\n","    x = Conv2D(32, (5, 5), activation='relu')(x)\n","    x = MaxPool2D((2, 2))(x)\n","    x = Conv2D(16, (5, 5), activation='relu')(x)\n","    x = MaxPool2D((2, 2))(x)\n","    x = Flatten()(x)\n","    x = Dropout(0.5)(x)\n","    x = Dense(100, activation='relu')(x)\n","    y_pred_output = Dense(1)(x)\n","    nll = NLL(10.0, 10.0)(y_true_input, y_pred_output, Z_input)\n","    return Model(inputs=[input_layer, y_true_input, Z_input], outputs=nll)\n","\n","def cnn_lmmnn_inception():\n","    y_true_input = Input(shape=(1, ),)\n","    Z_input = Input(shape=(1, ), dtype=tf.int64)\n","    base = InceptionV3(weights='imagenet', include_top=False, input_shape = (IMG_WIDTH, IMG_WIDTH, 3))\n","    x = base.output\n","    x = GlobalAveragePooling2D()(x)\n","    y_pred_output = Dense(1)(x)\n","    nll = NLL(1.0, 1.0)(y_true_input, y_pred_output, Z_input)\n","    model = Model(inputs=[base.input, y_true_input, Z_input], outputs=nll)\n","    train_top = 58\n","    for layer in model.layers[:-train_top]:\n","        layer.trainable = False\n","    for layer in model.layers[-train_top:]:\n","        layer.trainable = True\n","    return model\n","\n","def cnn_embedding(n_cats, embed_dim):\n","    input_layer = Input((IMG_WIDTH, IMG_WIDTH, 3))\n","    Z_input = Input(shape=(1,))\n","    embed = Embedding(n_cats, embed_dim, input_length = 1)(Z_input)\n","    embed = Reshape(target_shape = (embed_dim,))(embed)\n","    x = Conv2D(32, (5, 5), activation='relu')(input_layer)\n","    x = MaxPool2D((2, 2))(x)\n","    x = Conv2D(64, (5, 5), activation='relu')(x)\n","    x = MaxPool2D((2, 2))(x)\n","    x = Conv2D(32, (5, 5), activation='relu')(x)\n","    x = MaxPool2D((2, 2))(x)\n","    x = Conv2D(16, (5, 5), activation='relu')(x)\n","    x = MaxPool2D((2, 2))(x)\n","    x = Flatten()(x)\n","    x = Dropout(0.5)(x)\n","    x = Dense(100, activation='relu')(x)\n","    concat = Concatenate()([x, embed])\n","    output = Dense(1)(concat)\n","    return Model(inputs=[input_layer, Z_input], outputs=output)\n","\n","def cnn_embedding_inception(n_cats, embed_dim):\n","    base = InceptionV3(weights='imagenet', include_top=False, input_shape = (IMG_WIDTH, IMG_WIDTH, 3))\n","    Z_input = Input(shape=(1,))\n","    embed = Embedding(n_cats, embed_dim, input_length = 1)(Z_input)\n","    embed = Reshape(target_shape = (embed_dim,))(embed)\n","    x = base.output\n","    x = GlobalAveragePooling2D()(x)\n","    concat = Concatenate()([x, embed])\n","    output = Dense(1)(concat)\n","    model = Model(inputs = [base.input, Z_input], outputs = output)\n","    train_top = 55\n","    for layer in model.layers[:-train_top]:\n","        layer.trainable = False\n","    for layer in model.layers[-train_top:]:\n","        layer.trainable = True\n","    return model\n","\n","def calc_b_hat(Z_train, y_train, y_pred_tr, n_cats, sig2e, sig2b):\n","    b_hat = []\n","    for i in range(n_cats):\n","        i_vec = Z_train == i\n","        n_i = i_vec.sum()\n","        if n_i > 0:\n","            y_bar_i = y_train[i_vec].mean()\n","            y_pred_i = y_pred_tr[i_vec].mean()\n","            # BP(b_i) = (n_i * sig2b / (sig2a + n_i * sig2b)) * (y_bar_i - y_pred_bar_i)\n","            b_i = n_i * sig2b * (y_bar_i - y_pred_i) / (sig2e + n_i * sig2b)\n","        else:\n","            b_i = 0\n","        b_hat.append(b_i)\n","    return np.array(b_hat)\n","\n","def custom_train_generator_lmmnn(train_generator, epochs):\n","    count = 0\n","    while True:\n","        if count == train_generator.n * epochs:\n","            train_generator.reset()\n","            break\n","        count += train_generator.batch_size\n","        data = train_generator.next()\n","        imgs = data[0]\n","        y_true = data[1][:, 0]\n","        Z = data[1][:, 1]\n","        yield [imgs, y_true, Z], None\n","\n","def custom_valid_generator_lmmnn(valid_generator, epochs):\n","    count = 0 \n","    while True:\n","        if count == valid_generator.n * epochs:\n","            valid_generator.reset()\n","            break\n","        count += valid_generator.batch_size\n","        data = valid_generator.next()\n","        imgs = data[0]\n","        y_true = data[1][:, 0]\n","        Z = data[1][:, 1]\n","        yield [imgs, y_true, Z], None\n","\n","def custom_test_generator_lmmnn(test_generator, epochs):\n","    count = 0 \n","    while True:\n","        if count == test_generator.n * epochs:\n","            test_generator.reset()\n","            break\n","        count += test_generator.batch_size\n","        data = test_generator.next()\n","        imgs = data[0]\n","        y_true = data[1][:, 0]\n","        Z = data[1][:, 1]\n","        yield [imgs, y_true, Z], None\n","\n","def custom_train_generator_embed(train_generator, epochs):\n","    count = 0 \n","    while True:\n","        if count == train_generator.n * epochs:\n","            train_generator.reset()\n","            break\n","        count += train_generator.batch_size\n","        data = train_generator.next()\n","        imgs = data[0]\n","        y_true = data[1][:, 0]\n","        Z = data[1][:, 1]\n","        yield [imgs, Z], y_true\n","\n","def custom_valid_generator_embed(valid_generator, epochs):\n","    count = 0 \n","    while True:\n","        if count == valid_generator.n * epochs:\n","            valid_generator.reset()\n","            break\n","        count += valid_generator.batch_size\n","        data = valid_generator.next()\n","        imgs = data[0]\n","        y_true = data[1][:, 0]\n","        Z = data[1][:, 1]\n","        yield [imgs, Z], y_true\n","\n","def custom_test_generator_embed(test_generator, epochs):\n","    count = 0 \n","    while True:\n","        if count == test_generator.n * epochs:\n","            test_generator.reset()\n","            break\n","        count += test_generator.batch_size\n","        data = test_generator.next()\n","        imgs = data[0]\n","        y_true = data[1][:, 0]\n","        Z = data[1][:, 1]\n","        yield [imgs, Z], y_true\n","\n","def factors(n):    # (cf. https://stackoverflow.com/a/15703327/849891)\n","    j = 2\n","    while n > 1:\n","        for i in range(j, int(np.sqrt(n+0.05)) + 1):\n","            if n % i == 0:\n","                n /= i ; j = i\n","                yield i\n","                break\n","        else:\n","            if n > 1:\n","                yield n; break\n","\n","def get_batchsize_steps(n):\n","    factors_n = list(factors(n))\n","    if len(factors_n) > 1:\n","        batch_size = factors_n[-2]\n","    else:\n","        batch_size = 1\n","    steps = n // batch_size\n","    return batch_size, steps\n","\n","def get_generators(images_df, images_dir, train_samp_subj, valid_samp_subj, test_samp_subj, batch_size, reg_type):\n","    train_datagen = ImageDataGenerator(rescale = 1./255) # preprocessing_function = preprocess_input # for inception\n","    valid_datagen = ImageDataGenerator(rescale = 1./255) # preprocessing_function = preprocess_input # for inception\n","    test_datagen = ImageDataGenerator(rescale = 1./255) # preprocessing_function = preprocess_input # for inception\n","    if reg_type == 'ignore':\n","        y_cols = ['age']\n","    else:\n","        y_cols = ['age', 'subject_id2']\n","    train_generator = train_datagen.flow_from_dataframe(\n","        images_df[images_df['subject_id2'].isin(train_samp_subj)],\n","        directory = images_dir,\n","        x_col = 'image_id',\n","        y_col = y_cols,\n","        target_size = (IMG_WIDTH, IMG_WIDTH),\n","        class_mode = 'raw',\n","        batch_size = batch_size,\n","        shuffle = True,\n","        validate_filenames = False\n","    )\n","    valid_generator = valid_datagen.flow_from_dataframe(\n","        images_df[images_df['subject_id2'].isin(valid_samp_subj)],\n","        directory = images_dir,\n","        x_col = 'image_id',\n","        y_col = y_cols,\n","        target_size = (IMG_WIDTH, IMG_WIDTH),\n","        class_mode = 'raw',\n","        batch_size = batch_size,\n","        shuffle = False,\n","        validate_filenames = False\n","    )\n","    test_generator = test_datagen.flow_from_dataframe(\n","        images_df[images_df['subject_id2'].isin(test_samp_subj)],\n","        directory = images_dir,\n","        x_col = 'image_id',\n","        y_col = y_cols,\n","        target_size = (IMG_WIDTH, IMG_WIDTH),\n","        class_mode = 'raw',\n","        batch_size = batch_size,\n","        shuffle = False,\n","        validate_filenames = False\n","    )\n","    return train_generator, valid_generator, test_generator\n","\n","def reg_nn_ignore(train_generator, valid_generator, test_generator, n_cats, epochs, patience):\n","    model = cnn_ignore()\n","    model.compile(loss='mse', optimizer='adam')\n","    callbacks = [EarlyStopping(monitor='val_loss', patience=epochs if patience is None else patience)]\n","    model.fit(train_generator, validation_data = valid_generator, epochs=epochs, callbacks=callbacks, verbose=1)\n","    y_pred = model.predict(test_generator, verbose=1).reshape(test_generator.n)\n","    return y_pred, (None, None)\n","\n","def reg_nn_lmm(train_generator, valid_generator, test_generator, n_cats, epochs, patience):\n","    model = cnn_lmmnn()\n","    model.compile(optimizer= 'adam')\n","    \n","    patience = epochs if patience is None else patience\n","    callbacks = [EarlyStoppingWithSigmasConvergence(patience=patience), PrintSigmas()]\n","    step_size_train = train_generator.n // train_generator.batch_size\n","    step_size_valid = valid_generator.n // valid_generator.batch_size\n","    model.fit(custom_train_generator_lmmnn(train_generator, epochs), steps_per_epoch = step_size_train,\n","        validation_data = custom_valid_generator_lmmnn(valid_generator, epochs), validation_steps = step_size_valid,\n","        epochs=epochs, callbacks=callbacks, verbose=1)\n","    \n","    sig2e_est, sig2b_est = model.layers[-1].get_vars()\n","    \n","    batch_size_train, steps_train = get_batchsize_steps(train_generator.n)\n","    train_generator.reset()\n","    train_generator.batch_size = batch_size_train\n","    y_pred_tr = model.predict(custom_train_generator_lmmnn(train_generator, 1),\n","                              steps = steps_train,\n","                              verbose=1).reshape(train_generator.n)\n","    y_train = train_generator.labels[:, 0]\n","    Z_train = train_generator.labels[:, 1].astype(np.int)\n","    Z_test = test_generator.labels[:, 1].astype(np.int)\n","    b_hat = calc_b_hat(Z_train, y_train, y_pred_tr, n_cats, sig2e_est, sig2b_est)\n","    batch_size_test, steps_test = get_batchsize_steps(test_generator.n)\n","    test_generator.batch_size = batch_size_test\n","    y_pred = model.predict(custom_test_generator_lmmnn(test_generator, 1),\n","                           steps = steps_test, verbose=0).reshape(test_generator.n) + b_hat[Z_test]\n","    return y_pred, (sig2e_est, sig2b_est)\n","\n","def reg_nn_embed(train_generator, valid_generator, test_generator, n_cats, epochs, patience):\n","    embed_dim = 10\n","    model = cnn_embedding(n_cats, embed_dim)\n","    model.compile(loss='mse', optimizer='adam')\n","    callbacks = [EarlyStopping(monitor='val_loss', patience=epochs if patience is None else patience)]\n","    \n","    step_size_train = train_generator.n // train_generator.batch_size\n","    step_size_valid = valid_generator.n // valid_generator.batch_size\n","    \n","    model.fit(custom_train_generator_embed(train_generator, epochs), steps_per_epoch = step_size_train,\n","        validation_data = custom_valid_generator_embed(valid_generator, epochs),\n","        validation_steps = step_size_valid,\n","        epochs=epochs, callbacks=callbacks, verbose=1)\n","    batch_size_test, steps_test = get_batchsize_steps(test_generator.n)\n","    test_generator.batch_size = batch_size_test\n","    y_pred = model.predict(custom_test_generator_embed(test_generator, 1),\n","                           steps = steps_test, verbose=0).reshape(test_generator.n)\n","    return y_pred, (None, None)\n","        \n","def reg_nn(images_df, images_dir, train_samp_subj, valid_samp_subj, test_samp_subj,\n","    n_cats, batch_size=20, epochs=100, patience=10, reg_type='ignore'):\n","    train_generator, valid_generator, test_generator = get_generators(\n","        images_df, images_dir, train_samp_subj, valid_samp_subj, test_samp_subj, batch_size, reg_type)\n","    \n","    if reg_type == 'ignore':\n","        y_pred, sigmas = reg_nn_ignore(train_generator, valid_generator, test_generator, n_cats, epochs, patience)\n","    elif reg_type == 'lmm':\n","        y_pred, sigmas = reg_nn_lmm(train_generator, valid_generator, test_generator, n_cats, epochs, patience)\n","    else:\n","        y_pred, sigmas = reg_nn_embed(train_generator, valid_generator, test_generator, n_cats, epochs, patience)\n","    y_test = test_generator.labels[:, 0]\n","    mse = np.mean((y_pred - y_test)**2)\n","    mae = np.mean(np.abs(y_pred - y_test))\n","    return mse, mae, sigmas\n","\n","def iterate_reg_types(images_df, images_dir, res_df, counter, n_cats, train_samp_subj, valid_samp_subj, test_samp_subj,\n","    out_file, batch_size, epochs, patience):\n","        mse_lmm, mae_lmm, sigmas = reg_nn(images_df, images_dir, train_samp_subj, valid_samp_subj, test_samp_subj, n_cats,\n","            batch_size=batch_size, epochs=epochs, patience=patience, reg_type='lmm')\n","        print(' finished lmm, mse: %.2f, mae: %.2f' % (mse_lmm, mae_lmm))\n","        mse_ig, mae_ig, _ = reg_nn(images_df, images_dir, train_samp_subj, valid_samp_subj, test_samp_subj, n_cats,\n","            batch_size=batch_size, epochs=epochs, patience=patience, reg_type='ignore')\n","        print(' finished ignore, mse: %.2f, mae: %.2f' % (mse_ig, mae_ig))\n","        mse_em, mae_em, _ = reg_nn(images_df, images_dir, train_samp_subj, valid_samp_subj, test_samp_subj, n_cats,\n","            batch_size=batch_size, epochs=epochs, patience=patience, reg_type='embed')\n","        print(' finished embed, mse: %.2f, mae: %.2f' % (mse_em, mae_em))\n","        res_df.loc[next(counter)] = ['ignore', mse_ig, mae_ig, np.nan, np.nan]\n","        res_df.loc[next(counter)] = ['lmm', mse_lmm, mae_lmm, sigmas[0], sigmas[1]]\n","        res_df.loc[next(counter)] = ['embed', mse_em, mae_em, np.nan, np.nan]\n","        mse_dec = 100 * (mse_lmm - mse_ig) / mse_ig\n","        print('mse change from mse_ig: %.2f%%' % mse_dec)\n","        res_df.to_csv(out_file)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NMUSQIdbO38e","outputId":"7edd1994-e239-43fe-eeaa-eabb4d8db756"},"source":["for i, (train_index_subj, test_index_subj) in enumerate(kf.split(np.zeros(n_cats), np.zeros(n_cats))):\n","        print('iteration: %d' % i)\n","        train_index_subj, valid_index_subj = sample_split(i, train_index_subj)\n","        iterate_reg_types(images_df, images_dir, res_df, counter, n_cats, train_index_subj, valid_index_subj, test_index_subj,\n","            out_file, batch_size=batch_size, epochs=epochs, patience=patience)"],"execution_count":null,"outputs":[]}]}