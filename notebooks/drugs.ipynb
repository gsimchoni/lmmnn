{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.7"
    },
    "colab": {
      "name": "Drugs_Ratings_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "module_path = os.path.abspath(os.path.join('..'))\n",
        "if module_path not in sys.path:\n",
        "    sys.path.append(module_path)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nz7b58k2WgZg"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from lmmnn.layers import NLL\n",
        "from lmmnn.callbacks import EarlyStoppingWithSigmasConvergence\n",
        "from lmmnn.menet import menet_fit, menet_predict\n",
        "\n",
        "from tensorflow.keras.preprocessing import text, sequence\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Reshape, Concatenate, Input, Layer, Dropout, Flatten\n",
        "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
        "import tensorflow.keras.backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_bWJVbwWgZg"
      },
      "source": [
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "for gpu in gpus:\n",
        "    tf.config.experimental.set_memory_growth(gpu, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsV53IjcawCl"
      },
      "source": [
        "# The drugs_df CSV comes from simple binding the train and test TSVs from Gräßer et al. (2018),\n",
        "# available in the UCI ML repo, see our paper.\n",
        "drugs = pd.read_csv('drugs_df.csv')\n",
        "RE_col = 'drug_name'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "3k-26fe-WgZh",
        "outputId": "7ae790f7-947b-4d13-ede7-02b982bf42b3"
      },
      "source": [
        "drugs.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>drugName</th>\n",
              "      <th>condition</th>\n",
              "      <th>review</th>\n",
              "      <th>rating</th>\n",
              "      <th>date</th>\n",
              "      <th>usefulCount</th>\n",
              "      <th>drug_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>206461</td>\n",
              "      <td>Valsartan</td>\n",
              "      <td>Left Ventricular Dysfunction</td>\n",
              "      <td>\"It has no side effect, I take it in combinati...</td>\n",
              "      <td>9</td>\n",
              "      <td>May 20, 2012</td>\n",
              "      <td>27</td>\n",
              "      <td>3428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>95260</td>\n",
              "      <td>Guanfacine</td>\n",
              "      <td>ADHD</td>\n",
              "      <td>\"My son is halfway through his fourth week of ...</td>\n",
              "      <td>8</td>\n",
              "      <td>April 27, 2010</td>\n",
              "      <td>192</td>\n",
              "      <td>1542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>92703</td>\n",
              "      <td>Lybrel</td>\n",
              "      <td>Birth Control</td>\n",
              "      <td>\"I used to take another oral contraceptive, wh...</td>\n",
              "      <td>5</td>\n",
              "      <td>December 14, 2009</td>\n",
              "      <td>17</td>\n",
              "      <td>1989</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>138000</td>\n",
              "      <td>Ortho Evra</td>\n",
              "      <td>Birth Control</td>\n",
              "      <td>\"This is my first time using any form of birth...</td>\n",
              "      <td>8</td>\n",
              "      <td>November 3, 2015</td>\n",
              "      <td>10</td>\n",
              "      <td>2456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>35696</td>\n",
              "      <td>Buprenorphine / naloxone</td>\n",
              "      <td>Opiate Dependence</td>\n",
              "      <td>\"Suboxone has completely turned my life around...</td>\n",
              "      <td>9</td>\n",
              "      <td>November 27, 2016</td>\n",
              "      <td>37</td>\n",
              "      <td>553</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       id                  drugName  ... usefulCount drug_name\n",
              "0  206461                 Valsartan  ...          27      3428\n",
              "1   95260                Guanfacine  ...         192      1542\n",
              "2   92703                    Lybrel  ...          17      1989\n",
              "3  138000                Ortho Evra  ...          10      2456\n",
              "4   35696  Buprenorphine / naloxone  ...          37       553\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtxphUSvWgZj"
      },
      "source": [
        "max_features = 10000\n",
        "batch_size = 20\n",
        "epochs = 10\n",
        "seq_len = 100\n",
        "words_embed_dim = 100\n",
        "Z_embed_dim = 10\n",
        "lstm_kernels = 64\n",
        "n_cats = drugs[RE_col].max() + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqg-BtSgWgZj",
        "outputId": "5eb3e3e9-a980-4051-a3fd-118d00e6be3c"
      },
      "source": [
        "n_cats"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3671"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pN2l6DPWWgZk",
        "outputId": "a024b5a3-b248-4127-ef47-8ab0996ea2ed"
      },
      "source": [
        "drugs[RE_col].min()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nn4vTU8cWgZk",
        "outputId": "ef020809-6d9e-405f-8e04-ddfd03bf452e"
      },
      "source": [
        "drugs[RE_col].max()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3670"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWNy52KjWgZk"
      },
      "source": [
        "tokenizer = text.Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(drugs['review'])\n",
        "text_sequences = tokenizer.texts_to_sequences(drugs['review'])\n",
        "X = sequence.pad_sequences(text_sequences, padding='post', maxlen=seq_len)\n",
        "X = pd.DataFrame(X)\n",
        "x_cols = ['X' + str(i) for i in range(seq_len)]\n",
        "X.columns = x_cols\n",
        "X = pd.concat([X, drugs[RE_col]], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8ftYpoyWgZk",
        "outputId": "35404164-c853-4dd5-95da-dbdd3ff15831"
      },
      "source": [
        "X.loc[0, x_cols].values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   5,   38,   28,   35,  198,    1,   45,    5,   15,  832,   12,\n",
              "       2948,   99,  149,    2, 3852, 1585,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGLYZYXUWgZl",
        "outputId": "592c3e48-8c4b-41ac-e530-3925d6166302"
      },
      "source": [
        "((X[x_cols] > 0).astype(int).sum(axis=1) == seq_len).mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.427009759930811"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lJ4pVhZ1WgZl",
        "outputId": "4fcf6145-e9c1-4bac-8ec1-79a2390aedc4"
      },
      "source": [
        "drugs.loc[0, 'review']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\"It has no side effect, I take it in combination of Bystolic 5 Mg and Fish Oil\"'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNRTKb4VWgZl",
        "outputId": "de37f9e5-7f09-4ca5-840d-de692da53f7b"
      },
      "source": [
        "tokenizer.word_index['it']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3Q35JCiWgZl"
      },
      "source": [
        "def lstm_ignore():\n",
        "    input_layer = Input(shape=(None, ), dtype=tf.int32)\n",
        "    x = Embedding(max_features + 1, words_embed_dim)(input_layer)\n",
        "    x = LSTM(lstm_kernels)(x)\n",
        "    output = Dense(1)(x)\n",
        "    return Model(inputs=[input_layer], outputs=output)\n",
        "\n",
        "def lstm_lmmnn():\n",
        "    input_layer = Input(shape=(seq_len, ), dtype=tf.int32)\n",
        "    y_true_input = Input(shape=(1, ),)\n",
        "    Z_input = Input(shape=(1, ), dtype=tf.int64)\n",
        "    x = Embedding(max_features + 1, words_embed_dim)(input_layer)\n",
        "    x = LSTM(lstm_kernels)(x)\n",
        "    y_pred_output = Dense(1)(x)\n",
        "    nll = NLL(1.0, 1.0)(y_true_input, y_pred_output, Z_input)\n",
        "    return Model(inputs=[input_layer, y_true_input, Z_input], outputs=nll)\n",
        "\n",
        "def lstm_embed():\n",
        "    input_layer = Input(shape=(None, ), dtype=tf.int32)\n",
        "    Z_input = Input(shape=(1,))\n",
        "    embed = Embedding(n_cats, Z_embed_dim, input_length = 1)(Z_input)\n",
        "    embed = Reshape(target_shape = (Z_embed_dim, ))(embed)\n",
        "    x = Embedding(max_features + 1, words_embed_dim)(input_layer)\n",
        "    x = LSTM(lstm_kernels)(x)\n",
        "    concat = Concatenate()([x, embed])\n",
        "    output = Dense(1)(concat)\n",
        "    return Model(inputs=[input_layer, Z_input], outputs=output)\n",
        "\n",
        "def lstm_ohe(p):\n",
        "    input_layer = Input(shape=(None, ), dtype=tf.int32)\n",
        "    ohe_input = Input(shape=(p, ))\n",
        "    x = Embedding(max_features + 1, words_embed_dim)(input_layer)\n",
        "    x = LSTM(lstm_kernels)(x)\n",
        "    concat = Concatenate()([x, ohe_input])\n",
        "    output = Dense(1)(concat)\n",
        "    return Model(inputs=[input_layer, ohe_input], outputs=output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaqoyIpBWgZm"
      },
      "source": [
        "def calc_b_hat(Z_train, y_train, y_pred_tr, n_cats, sig2e, sig2b):\n",
        "    b_hat = []\n",
        "    for i in range(n_cats):\n",
        "        i_vec = Z_train == i\n",
        "        n_i = i_vec.sum()\n",
        "        if n_i > 0:\n",
        "            y_bar_i = y_train[i_vec].mean()\n",
        "            y_pred_i = y_pred_tr[i_vec].mean()\n",
        "            # BP(b_i) = (n_i * sig2b / (sig2a + n_i * sig2b)) * (y_bar_i - y_pred_bar_i)\n",
        "            b_i = n_i * sig2b * (y_bar_i - y_pred_i) / (sig2e + n_i * sig2b)\n",
        "        else:\n",
        "            b_i = 0\n",
        "        b_hat.append(b_i)\n",
        "    return np.array(b_hat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPnvvceLWgZm"
      },
      "source": [
        "def process_one_hot_encoding(X_train, X_test, RE_col):\n",
        "    X_train_ohe = pd.concat([X_train[x_cols], pd.get_dummies(X_train[RE_col])], axis=1)\n",
        "    X_test_ohe = pd.concat([X_test[x_cols], pd.get_dummies(X_test[RE_col])], axis=1)\n",
        "    X_test_cols_in_train = set(X_test_ohe.columns).intersection(X_train_ohe.columns)\n",
        "    X_train_cols_not_in_test = set(X_train_ohe.columns).difference(X_test_ohe.columns)\n",
        "    X_test_comp = pd.DataFrame(np.zeros((X_test.shape[0], len(X_train_cols_not_in_test))),\n",
        "                               columns=X_train_cols_not_in_test, dtype=np.uint8, index=X_test.index)\n",
        "    X_test_ohe_comp = pd.concat([X_test_ohe[X_test_cols_in_train], X_test_comp], axis=1)\n",
        "    X_test_ohe_comp = X_test_ohe_comp[X_train_ohe.columns]\n",
        "    return X_train_ohe, X_test_ohe_comp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1vjzVMuWgZn"
      },
      "source": [
        "def reg_nn_ignore(X_train, X_test, y_train, y_test, n_cats, batch_size, epochs, patience, deep=False):\n",
        "    model = lstm_ignore()\n",
        "    model.compile(loss='mse', optimizer='adam')\n",
        "\n",
        "    callbacks = [EarlyStopping(monitor='val_loss', patience=epochs if patience is None else patience)]\n",
        "    history = model.fit(X_train[x_cols], y_train, batch_size=batch_size, epochs=epochs,\n",
        "                        validation_split=0.1, callbacks=callbacks, verbose=1)\n",
        "    y_pred = model.predict(X_test[x_cols]).reshape(X_test.shape[0])\n",
        "    y_pred = np.clip(y_pred, 1, 10)\n",
        "    return y_pred, (None, None)\n",
        "\n",
        "def reg_nn_ohe(X_train, X_test, y_train, y_test, n_cats, batch_size, epochs, patience, deep=False):\n",
        "    X_train, X_test = process_one_hot_encoding(X_train, X_test, RE_col)\n",
        "    model = lstm_ohe(X_train.drop(x_cols, axis=1).shape[1])\n",
        "    model.compile(loss='mse', optimizer='adam')\n",
        "\n",
        "    callbacks = [EarlyStopping(monitor='val_loss', patience=epochs if patience is None else patience)]\n",
        "    history = model.fit([X_train[x_cols], X_train.drop(x_cols, axis=1)], y_train, batch_size=batch_size, epochs=epochs,\n",
        "                        validation_split=0.1, callbacks=callbacks, verbose=1)\n",
        "    y_pred = model.predict([X_test[x_cols], X_test.drop(x_cols, axis=1)]).reshape(X_test.shape[0])\n",
        "    y_pred = np.clip(y_pred, 1, 10)\n",
        "    return y_pred, (None, None)\n",
        "\n",
        "def reg_nn_lmm(X_train, X_test, y_train, y_test, n_cats, batch_size, epochs, patience, deep=False):\n",
        "    model = lstm_lmmnn()\n",
        "    model.compile(optimizer= 'adam')\n",
        "    \n",
        "    patience = epochs if patience is None else patience\n",
        "    callbacks = [EarlyStoppingWithSigmasConvergence(patience=patience), PrintSigmas()]\n",
        "    history = model.fit([X_train[x_cols], y_train, X_train[RE_col]], None,\n",
        "                        batch_size=batch_size, epochs=epochs, validation_split=0.1,\n",
        "                        callbacks=callbacks, verbose=1)\n",
        "    \n",
        "    sig2e_est, sig2b_est = model.layers[-1].get_vars()\n",
        "    y_pred_tr = model.predict([X_train[x_cols], y_train, X_train[RE_col]]).reshape(X_train.shape[0])\n",
        "    y_pred_tr = np.clip(y_pred_tr, 1, 10)\n",
        "    b_hat = calc_b_hat(X_train[RE_col], y_train, y_pred_tr, n_cats, sig2e_est, sig2b_est)\n",
        "    dummy_y_test = np.random.normal(size=y_test.shape)\n",
        "    y_pred = model.predict([X_test[x_cols], dummy_y_test, X_test[RE_col]]).reshape(X_test.shape[0]) + b_hat[X_test[RE_col]]\n",
        "    y_pred = np.clip(y_pred, 1, 10)\n",
        "    return y_pred, (sig2e_est, sig2b_est)\n",
        "\n",
        "def reg_nn_embed(X_train, X_test, y_train, y_test, n_cats, batch_size, epochs, patience, deep=False):\n",
        "    model = lstm_embed()\n",
        "\n",
        "    model.compile(loss='mse', optimizer='adam')\n",
        "\n",
        "    callbacks = [EarlyStopping(monitor='val_loss', patience=epochs if patience is None else patience)]\n",
        "    history = model.fit([X_train[x_cols], X_train[RE_col]], y_train,\n",
        "                        batch_size=batch_size, epochs=epochs, validation_split=0.1,\n",
        "                        callbacks=callbacks, verbose=1)\n",
        "    y_pred = model.predict([X_test[x_cols], X_test[RE_col]]).reshape(X_test.shape[0])\n",
        "    y_pred = np.clip(y_pred, 1, 10)\n",
        "    return y_pred, (None, None)\n",
        "\n",
        "def reg_nn_menet(X_train, X_test, y_train, y_test, n_cats, batch_size, epochs, patience, deep=False):\n",
        "    q = n_cats\n",
        "    clusters_train, clusters_test = X_train[RE_col].values, X_test[RE_col].values\n",
        "    X_train, X_test = X_train[x_cols].values, X_test[x_cols].values\n",
        "    y_train, y_test = y_train.values, y_test.values\n",
        "\n",
        "    model = lstm_ignore()\n",
        "    model.compile(loss='mse', optimizer='adam')\n",
        "\n",
        "    model, b_hat, sig2e_est, n_epochs, _ = menet_fit(model, X_train, y_train, clusters_train, q,\n",
        "        batch_size, epochs, patience, verbose=True)\n",
        "    y_pred = menet_predict(model, X_test, clusters_test, q, b_hat)\n",
        "    y_pred = np.clip(y_pred, 1, 10)\n",
        "    return y_pred, (sig2e_est, None), n_epochs\n",
        "\n",
        "def reg_nn(X_train, X_test, y_train, y_test, n_cats, batch=30, epochs=100, patience=5, reg_type='ohe', deep=False):    \n",
        "    if reg_type == 'ohe':\n",
        "        y_pred, sigmas = reg_nn_ohe(X_train, X_test, y_train, y_test, n_cats, batch, epochs, patience, deep)\n",
        "    elif reg_type == 'lmm':\n",
        "        y_pred, sigmas = reg_nn_lmm(X_train, X_test, y_train, y_test, n_cats, batch, epochs, patience, deep)\n",
        "    elif reg_type == 'ignore':\n",
        "        y_pred, sigmas = reg_nn_ignore(X_train, X_test, y_train, y_test, n_cats, batch, epochs, patience, deep)\n",
        "    elif reg_type == 'embed':\n",
        "        y_pred, sigmas = reg_nn_embed(X_train, X_test, y_train, y_test, n_cats, batch, epochs, patience, deep)\n",
        "    elif reg_type == 'menet':\n",
        "        y_pred, sigmas, _ = reg_nn_menet(X_train, X_test, y_train, y_test, n_cats, batch, epochs, patience, deep)\n",
        "    else:\n",
        "        raise ValueError(reg_type + ' is an unknown reg_type')\n",
        "    mse = np.mean((y_pred - y_test)**2)\n",
        "    return mse, sigmas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "l-LrfWXYWgZp",
        "outputId": "80aa6a65-7d16-4bb0-906a-fdb458d146f7"
      },
      "source": [
        "res = pd.DataFrame(columns=['experiment', 'exp_type', 'deep', 'mse', 'sigma_e_est', 'sigma_b_est'])\n",
        "counter = 0\n",
        "\n",
        "def iterate_reg_types(X_train, X_test, y_train, y_test, deep=True):\n",
        "    global counter\n",
        "    mse_ig, _ = reg_nn(X_train, X_test, y_train, y_test, n_cats, reg_type='ignore', deep=deep)\n",
        "    print(' finished ignore deep=%s, mse: %.2f' % (deep, mse_ig))\n",
        "    mse_lmm, sigmas = reg_nn(X_train, X_test, y_train, y_test, n_cats, reg_type='lmm', deep=deep)\n",
        "    print(' finished lmm deep=%s, mse: %.2f' % (deep, mse_lmm))\n",
        "    mse_lm, _ = reg_nn(X_train, X_test, y_train, y_test, n_cats, reg_type='ohe', deep=deep)\n",
        "    print(' finished lm deep=%s, mse: %.2f' % (deep, mse_lm))\n",
        "    mse_em, _ = reg_nn(X_train, X_test, y_train, y_test, n_cats, reg_type='embed', deep=deep)\n",
        "    print(' finished embed deep=%s, mse: %.2f' % (deep, mse_em))\n",
        "    mse_me, sigmas_me = reg_nn(X_train, X_test, y_train, y_test, n_cats, reg_type='menet', deep=deep)\n",
        "    print(' finished menet deep=%s, mse: %.2f' % (deep, mse_me))\n",
        "    mse_dec = 100 * (mse_lmm - mse_lm) / mse_lm\n",
        "    res.loc[counter + 0] = [i, 'ohe', deep, mse_lm, np.nan, np.nan]\n",
        "    res.loc[counter + 1] = [i, 'lmm', deep, mse_lmm, sigmas[0], sigmas[1]]\n",
        "    res.loc[counter + 2] = [i, 'ignore', deep, mse_ig, np.nan, np.nan]\n",
        "    res.loc[counter + 3] = [i, 'embed', deep, mse_em, np.nan, np.nan]\n",
        "    res.loc[counter + 4] = [i, 'menet', deep, mse_me, sigmas_me[0], np.nan]\n",
        "    counter += 5\n",
        "    print('iteration %d, deep=%s, mse change from mse_lm: %.2f%%' % (i, deep, mse_dec)) \n",
        "\n",
        "kf = KFold(n_splits=5)\n",
        "y = drugs['rating']\n",
        "\n",
        "for i, (train_index, test_index) in enumerate(kf.split(X, y)):\n",
        "    print('iteration %d' % i)\n",
        "    X_train, X_test, y_train, y_test = X.loc[train_index], X.loc[test_index], y[train_index], y[test_index]\n",
        "    iterate_reg_types(X_train, X_test, y_train, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration 0\n",
            "Epoch 1/100\n",
            "5162/5162 [==============================] - 100s 16ms/step - loss: 10.1114 - val_loss: 5.1541\n",
            "Epoch 2/100\n",
            "5162/5162 [==============================] - 83s 16ms/step - loss: 4.2959 - val_loss: 4.2942\n",
            "Epoch 3/100\n",
            "5162/5162 [==============================] - 83s 16ms/step - loss: 3.3876 - val_loss: 3.9637\n",
            "Epoch 4/100\n",
            "5162/5162 [==============================] - 84s 16ms/step - loss: 2.7585 - val_loss: 3.7898\n",
            "Epoch 5/100\n",
            "5162/5162 [==============================] - 86s 17ms/step - loss: 2.3248 - val_loss: 3.6379\n",
            "Epoch 6/100\n",
            "5162/5162 [==============================] - 87s 17ms/step - loss: 1.9747 - val_loss: 3.4619\n",
            "Epoch 7/100\n",
            "5162/5162 [==============================] - 85s 16ms/step - loss: 1.6714 - val_loss: 3.3855\n",
            "Epoch 8/100\n",
            "5162/5162 [==============================] - 82s 16ms/step - loss: 1.4724 - val_loss: 3.3200\n",
            "Epoch 9/100\n",
            "5162/5162 [==============================] - 82s 16ms/step - loss: 1.2893 - val_loss: 3.2585\n",
            "Epoch 10/100\n",
            "5162/5162 [==============================] - 83s 16ms/step - loss: 1.1268 - val_loss: 3.2836\n",
            "Epoch 11/100\n",
            "5162/5162 [==============================] - 85s 16ms/step - loss: 0.9866 - val_loss: 3.2081\n",
            "Epoch 12/100\n",
            "5162/5162 [==============================] - 86s 17ms/step - loss: 0.8996 - val_loss: 3.1692\n",
            "Epoch 13/100\n",
            "5162/5162 [==============================] - 86s 17ms/step - loss: 0.7858 - val_loss: 3.2125\n",
            "Epoch 14/100\n",
            "5162/5162 [==============================] - 86s 17ms/step - loss: 0.7091 - val_loss: 3.0489\n",
            "Epoch 15/100\n",
            "5162/5162 [==============================] - 86s 17ms/step - loss: 0.6595 - val_loss: 3.0563\n",
            "Epoch 16/100\n",
            "5162/5162 [==============================] - 86s 17ms/step - loss: 0.5935 - val_loss: 3.0579\n",
            "Epoch 17/100\n",
            "5162/5162 [==============================] - 86s 17ms/step - loss: 0.5334 - val_loss: 3.0369\n",
            "Epoch 18/100\n",
            "5162/5162 [==============================] - 86s 17ms/step - loss: 0.4880 - val_loss: 2.9936\n",
            "Epoch 19/100\n",
            "5162/5162 [==============================] - 87s 17ms/step - loss: 0.4561 - val_loss: 2.9649\n",
            "Epoch 20/100\n",
            "5162/5162 [==============================] - 87s 17ms/step - loss: 0.4213 - val_loss: 2.9857\n",
            "Epoch 21/100\n",
            "5162/5162 [==============================] - 87s 17ms/step - loss: 0.3906 - val_loss: 2.9632\n",
            "Epoch 22/100\n",
            "5162/5162 [==============================] - 88s 17ms/step - loss: 0.3609 - val_loss: 2.9437\n",
            "Epoch 23/100\n",
            "5162/5162 [==============================] - 85s 16ms/step - loss: 0.3363 - val_loss: 2.9811\n",
            "Epoch 24/100\n",
            "5162/5162 [==============================] - 86s 17ms/step - loss: 0.3219 - val_loss: 2.9703\n",
            "Epoch 25/100\n",
            "5162/5162 [==============================] - 86s 17ms/step - loss: 0.3026 - val_loss: 2.8854\n",
            "Epoch 26/100\n",
            "5162/5162 [==============================] - 88s 17ms/step - loss: 0.2856 - val_loss: 2.9881\n",
            "Epoch 27/100\n",
            "5162/5162 [==============================] - 87s 17ms/step - loss: 0.2657 - val_loss: 2.9408\n",
            "Epoch 28/100\n",
            "5162/5162 [==============================] - 85s 17ms/step - loss: 0.2608 - val_loss: 2.9545\n",
            "Epoch 29/100\n",
            "5162/5162 [==============================] - 83s 16ms/step - loss: 0.2416 - val_loss: 2.8531\n",
            "Epoch 30/100\n",
            "5162/5162 [==============================] - 86s 17ms/step - loss: 0.2372 - val_loss: 2.9444\n",
            "Epoch 31/100\n",
            "5162/5162 [==============================] - 88s 17ms/step - loss: 0.2273 - val_loss: 2.8802\n",
            "Epoch 32/100\n",
            "5162/5162 [==============================] - 86s 17ms/step - loss: 0.2180 - val_loss: 2.8885\n",
            "Epoch 33/100\n",
            "5162/5162 [==============================] - 86s 17ms/step - loss: 0.2065 - val_loss: 2.9368\n",
            "Epoch 34/100\n",
            "5162/5162 [==============================] - 90s 17ms/step - loss: 0.1949 - val_loss: 2.8867\n",
            " finished ignore deep=True, mse: 2.78\n",
            "Epoch 1/100\n",
            "5162/5162 [==============================] - 104s 20ms/step - loss: 101.0159 - val_loss: 66.6516\n",
            "Epoch 2/100\n",
            "5162/5162 [==============================] - 102s 20ms/step - loss: 64.7012 - val_loss: 64.8736\n",
            "Epoch 3/100\n",
            "5162/5162 [==============================] - 101s 20ms/step - loss: 61.5243 - val_loss: 63.9854\n",
            "Epoch 4/100\n",
            "5162/5162 [==============================] - 102s 20ms/step - loss: 58.3975 - val_loss: 63.8648\n",
            "Epoch 5/100\n",
            "5162/5162 [==============================] - 102s 20ms/step - loss: 55.9290 - val_loss: 63.6326\n",
            "Epoch 6/100\n",
            "5162/5162 [==============================] - 102s 20ms/step - loss: 53.5222 - val_loss: 64.4017\n",
            "Epoch 7/100\n",
            "5162/5162 [==============================] - 103s 20ms/step - loss: 51.1383 - val_loss: 65.0271\n",
            "Epoch 8/100\n",
            "5162/5162 [==============================] - 103s 20ms/step - loss: 49.1378 - val_loss: 66.3580\n",
            "Epoch 9/100\n",
            "5162/5162 [==============================] - 100s 19ms/step - loss: 47.1544 - val_loss: 68.2056\n",
            "Epoch 10/100\n",
            "5162/5162 [==============================] - 99s 19ms/step - loss: 45.2914 - val_loss: 69.9841\n",
            "Epoch 11/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 43.4969 - val_loss: 72.5205\n",
            " sig2e: 1.11, sig2b: 0.06\n",
            "Epoch 12/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 41.7275 - val_loss: 73.4696\n",
            "Epoch 13/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 40.0875 - val_loss: 80.0987\n",
            "Epoch 14/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 38.5222 - val_loss: 80.8066\n",
            "Epoch 15/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 37.1391 - val_loss: 85.5014\n",
            "Epoch 16/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 35.6447 - val_loss: 90.5617\n",
            "Epoch 17/100\n",
            "5162/5162 [==============================] - 102s 20ms/step - loss: 34.4062 - val_loss: 93.1424\n",
            "Epoch 18/100\n",
            "5162/5162 [==============================] - 102s 20ms/step - loss: 33.2078 - val_loss: 96.5293\n",
            "Epoch 19/100\n",
            "5162/5162 [==============================] - 100s 19ms/step - loss: 31.8158 - val_loss: 101.1920\n",
            "Epoch 20/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 30.8643 - val_loss: 114.7913\n",
            "Epoch 21/100\n",
            "5162/5162 [==============================] - 99s 19ms/step - loss: 29.3942 - val_loss: 116.8396\n",
            " sig2e: 0.44, sig2b: 0.02\n",
            "Epoch 22/100\n",
            "5162/5162 [==============================] - 99s 19ms/step - loss: 28.3573 - val_loss: 119.7332\n",
            "Epoch 23/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 27.4427 - val_loss: 123.9659\n",
            "Epoch 24/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 26.5064 - val_loss: 125.1054\n",
            "Epoch 25/100\n",
            "5162/5162 [==============================] - 99s 19ms/step - loss: 25.6510 - val_loss: 141.3472\n",
            "Epoch 26/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 24.6070 - val_loss: 152.9037\n",
            "Epoch 27/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 23.9224 - val_loss: 154.3858\n",
            "Epoch 28/100\n",
            "5162/5162 [==============================] - 99s 19ms/step - loss: 23.1644 - val_loss: 148.3027\n",
            "Epoch 29/100\n",
            "5162/5162 [==============================] - 99s 19ms/step - loss: 22.6502 - val_loss: 174.7436\n",
            "Epoch 30/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 21.7703 - val_loss: 173.3369\n",
            "Epoch 31/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 20.8747 - val_loss: 192.9076\n",
            " sig2e: 0.23, sig2b: 0.00\n",
            "Epoch 32/100\n",
            "5162/5162 [==============================] - 101s 20ms/step - loss: 19.8893 - val_loss: 195.0345\n",
            "Epoch 33/100\n",
            "5162/5162 [==============================] - 99s 19ms/step - loss: 19.7764 - val_loss: 176.2257\n",
            "Epoch 34/100\n",
            "5162/5162 [==============================] - 99s 19ms/step - loss: 19.1249 - val_loss: 188.6131\n",
            "Epoch 35/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 18.7072 - val_loss: 194.9963\n",
            "Epoch 36/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 17.6509 - val_loss: 222.7761\n",
            "Epoch 37/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 17.1265 - val_loss: 217.6617\n",
            "Epoch 38/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 16.8131 - val_loss: 231.9059\n",
            "Epoch 39/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 16.1343 - val_loss: 226.6245\n",
            "Epoch 40/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 15.6539 - val_loss: 225.9937\n",
            "Epoch 41/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 15.2860 - val_loss: 240.3403\n",
            " sig2e: 0.17, sig2b: 0.01\n",
            "Epoch 42/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 14.2089 - val_loss: 266.9888\n",
            "Epoch 43/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 15.1129 - val_loss: 250.6297\n",
            "Epoch 44/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 13.9465 - val_loss: 261.0294\n",
            "Epoch 45/100\n",
            "5162/5162 [==============================] - 99s 19ms/step - loss: 13.8256 - val_loss: 308.8195\n",
            "Epoch 46/100\n",
            "5162/5162 [==============================] - 99s 19ms/step - loss: 13.1520 - val_loss: 304.3705\n",
            "Epoch 47/100\n",
            "5162/5162 [==============================] - 99s 19ms/step - loss: 12.6675 - val_loss: 289.0726\n",
            "Epoch 48/100\n",
            "5162/5162 [==============================] - 99s 19ms/step - loss: 12.5642 - val_loss: 269.8842\n",
            "Epoch 49/100\n",
            "5162/5162 [==============================] - 103s 20ms/step - loss: 11.6391 - val_loss: 294.3328\n",
            "Epoch 50/100\n",
            "5162/5162 [==============================] - 103s 20ms/step - loss: 12.0227 - val_loss: 253.3383\n",
            "Epoch 51/100\n",
            "5162/5162 [==============================] - 106s 21ms/step - loss: 11.3381 - val_loss: 349.9584\n",
            " sig2e: 0.12, sig2b: 0.00\n",
            "Epoch 52/100\n",
            "5162/5162 [==============================] - 105s 20ms/step - loss: 11.0415 - val_loss: 288.8291\n",
            "Epoch 53/100\n",
            "5162/5162 [==============================] - 104s 20ms/step - loss: 10.6676 - val_loss: 262.1747\n",
            "Epoch 54/100\n",
            "5162/5162 [==============================] - 101s 20ms/step - loss: 11.1352 - val_loss: 348.3459\n",
            "Epoch 55/100\n",
            "5162/5162 [==============================] - 100s 19ms/step - loss: 10.4021 - val_loss: 337.7969\n",
            "Epoch 56/100\n",
            "5162/5162 [==============================] - 100s 19ms/step - loss: 10.2444 - val_loss: 278.1226\n",
            "Epoch 57/100\n",
            "5162/5162 [==============================] - 99s 19ms/step - loss: 9.3293 - val_loss: 397.4870\n",
            "Epoch 58/100\n",
            "5162/5162 [==============================] - 100s 19ms/step - loss: 9.3255 - val_loss: 414.4266\n",
            "Epoch 59/100\n",
            "5162/5162 [==============================] - 100s 19ms/step - loss: 9.2560 - val_loss: 397.5168\n",
            "Epoch 60/100\n",
            "5162/5162 [==============================] - 99s 19ms/step - loss: 8.1404 - val_loss: 391.6354\n",
            "Epoch 61/100\n",
            "5162/5162 [==============================] - 99s 19ms/step - loss: 9.0256 - val_loss: 367.7917\n",
            " sig2e: 0.11, sig2b: 0.01\n",
            "Epoch 62/100\n",
            "5162/5162 [==============================] - 99s 19ms/step - loss: 8.6337 - val_loss: 322.3886\n",
            "Epoch 63/100\n",
            "5162/5162 [==============================] - 100s 19ms/step - loss: 7.4210 - val_loss: 372.1735\n",
            "Epoch 64/100\n",
            "5162/5162 [==============================] - 99s 19ms/step - loss: 8.3665 - val_loss: 407.4916\n",
            "Epoch 65/100\n",
            "5162/5162 [==============================] - 99s 19ms/step - loss: 7.7148 - val_loss: 402.0410\n",
            "Epoch 66/100\n",
            "5162/5162 [==============================] - 99s 19ms/step - loss: 7.4505 - val_loss: 391.6556\n",
            "Epoch 67/100\n",
            "5162/5162 [==============================] - 99s 19ms/step - loss: 7.0029 - val_loss: 354.5652\n",
            "Epoch 68/100\n",
            "5162/5162 [==============================] - 99s 19ms/step - loss: 6.7677 - val_loss: 398.7822\n",
            " finished lmm deep=True, mse: 2.68\n",
            "Epoch 1/100\n",
            "5162/5162 [==============================] - 93s 18ms/step - loss: 11.5706 - val_loss: 4.9605\n",
            "Epoch 2/100\n",
            "5162/5162 [==============================] - 91s 18ms/step - loss: 4.4592 - val_loss: 4.2460\n",
            "Epoch 3/100\n",
            "5162/5162 [==============================] - 92s 18ms/step - loss: 3.4969 - val_loss: 4.0429\n",
            "Epoch 4/100\n",
            "5162/5162 [==============================] - 90s 18ms/step - loss: 2.9185 - val_loss: 3.7997\n",
            "Epoch 5/100\n",
            "5162/5162 [==============================] - 89s 17ms/step - loss: 2.4156 - val_loss: 3.6899\n",
            "Epoch 6/100\n",
            "5162/5162 [==============================] - 91s 18ms/step - loss: 2.0692 - val_loss: 3.5826\n",
            "Epoch 7/100\n",
            "5162/5162 [==============================] - 91s 18ms/step - loss: 1.7693 - val_loss: 3.4770\n",
            "Epoch 8/100\n",
            "5162/5162 [==============================] - 91s 18ms/step - loss: 1.5137 - val_loss: 3.3571\n",
            "Epoch 9/100\n",
            "5162/5162 [==============================] - 91s 18ms/step - loss: 1.3481 - val_loss: 3.2277\n",
            "Epoch 10/100\n",
            "5162/5162 [==============================] - 91s 18ms/step - loss: 1.1822 - val_loss: 3.2928\n",
            "Epoch 11/100\n",
            "5162/5162 [==============================] - 91s 18ms/step - loss: 1.0375 - val_loss: 3.1678\n",
            "Epoch 12/100\n",
            "5162/5162 [==============================] - 91s 18ms/step - loss: 0.9321 - val_loss: 3.1367\n",
            "Epoch 13/100\n",
            "5162/5162 [==============================] - 91s 18ms/step - loss: 0.8224 - val_loss: 3.1180\n",
            "Epoch 14/100\n",
            "5162/5162 [==============================] - 91s 18ms/step - loss: 0.7469 - val_loss: 3.0890\n",
            "Epoch 15/100\n",
            "5162/5162 [==============================] - 92s 18ms/step - loss: 0.6754 - val_loss: 3.0795\n",
            "Epoch 16/100\n",
            "5162/5162 [==============================] - 91s 18ms/step - loss: 0.6159 - val_loss: 3.0551\n",
            "Epoch 17/100\n",
            "5162/5162 [==============================] - 92s 18ms/step - loss: 0.5571 - val_loss: 3.0113\n",
            "Epoch 18/100\n",
            "5162/5162 [==============================] - 92s 18ms/step - loss: 0.5066 - val_loss: 2.9588\n",
            "Epoch 19/100\n",
            "5162/5162 [==============================] - 91s 18ms/step - loss: 0.4685 - val_loss: 2.9737\n",
            "Epoch 20/100\n",
            "5162/5162 [==============================] - 91s 18ms/step - loss: 0.4358 - val_loss: 3.0120\n",
            "Epoch 21/100\n",
            "5162/5162 [==============================] - 92s 18ms/step - loss: 0.4016 - val_loss: 2.9138\n",
            "Epoch 22/100\n",
            "5162/5162 [==============================] - 91s 18ms/step - loss: 0.3702 - val_loss: 2.9070\n",
            "Epoch 23/100\n",
            "5162/5162 [==============================] - 91s 18ms/step - loss: 0.3484 - val_loss: 2.9092\n",
            "Epoch 24/100\n",
            "5162/5162 [==============================] - 92s 18ms/step - loss: 0.3241 - val_loss: 2.9983\n",
            "Epoch 25/100\n",
            "5162/5162 [==============================] - 91s 18ms/step - loss: 0.3095 - val_loss: 2.8664\n",
            "Epoch 26/100\n",
            "5162/5162 [==============================] - 91s 18ms/step - loss: 0.2970 - val_loss: 2.9141\n",
            "Epoch 27/100\n",
            "5162/5162 [==============================] - 91s 18ms/step - loss: 0.2691 - val_loss: 2.8816\n",
            "Epoch 28/100\n",
            "5162/5162 [==============================] - 91s 18ms/step - loss: 0.2546 - val_loss: 2.8589\n",
            "Epoch 29/100\n",
            "5162/5162 [==============================] - 90s 17ms/step - loss: 0.2394 - val_loss: 2.8411\n",
            "Epoch 30/100\n",
            "5162/5162 [==============================] - 92s 18ms/step - loss: 0.2320 - val_loss: 2.8616\n",
            "Epoch 31/100\n",
            "5162/5162 [==============================] - 91s 18ms/step - loss: 0.2182 - val_loss: 2.8034\n",
            "Epoch 32/100\n",
            "5162/5162 [==============================] - 91s 18ms/step - loss: 0.2107 - val_loss: 2.7992\n",
            "Epoch 33/100\n",
            "5162/5162 [==============================] - 91s 18ms/step - loss: 0.2018 - val_loss: 2.8129\n",
            "Epoch 34/100\n",
            "5162/5162 [==============================] - 91s 18ms/step - loss: 0.1889 - val_loss: 2.8562\n",
            "Epoch 35/100\n",
            "5162/5162 [==============================] - 91s 18ms/step - loss: 0.1857 - val_loss: 2.7913\n",
            "Epoch 36/100\n",
            "5162/5162 [==============================] - 91s 18ms/step - loss: 0.1728 - val_loss: 2.8125\n",
            "Epoch 37/100\n",
            "5162/5162 [==============================] - 90s 17ms/step - loss: 0.1745 - val_loss: 2.8268\n",
            "Epoch 38/100\n",
            "5162/5162 [==============================] - 90s 18ms/step - loss: 0.1664 - val_loss: 2.8123\n",
            "Epoch 39/100\n",
            "5162/5162 [==============================] - 91s 18ms/step - loss: 0.1639 - val_loss: 2.7742\n",
            "Epoch 40/100\n",
            "5162/5162 [==============================] - 91s 18ms/step - loss: 0.1541 - val_loss: 2.8050\n",
            "Epoch 41/100\n",
            "5162/5162 [==============================] - 91s 18ms/step - loss: 0.1476 - val_loss: 2.7868\n",
            "Epoch 42/100\n",
            "5162/5162 [==============================] - 93s 18ms/step - loss: 0.1543 - val_loss: 2.8126\n",
            "Epoch 43/100\n",
            "5162/5162 [==============================] - 92s 18ms/step - loss: 0.1396 - val_loss: 2.8212\n",
            "Epoch 44/100\n",
            "5162/5162 [==============================] - 93s 18ms/step - loss: 0.1367 - val_loss: 2.8160\n",
            " finished lm deep=True, mse: 2.76\n",
            "Epoch 1/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 9.7277 - val_loss: 4.7511\n",
            "Epoch 2/100\n",
            "5162/5162 [==============================] - 96s 19ms/step - loss: 4.1443 - val_loss: 4.1961\n",
            "Epoch 3/100\n",
            "5162/5162 [==============================] - 96s 19ms/step - loss: 3.2637 - val_loss: 3.9926\n",
            "Epoch 4/100\n",
            "5162/5162 [==============================] - 96s 19ms/step - loss: 2.6592 - val_loss: 3.6822\n",
            "Epoch 5/100\n",
            "5162/5162 [==============================] - 97s 19ms/step - loss: 2.2490 - val_loss: 3.5949\n",
            "Epoch 6/100\n",
            "5162/5162 [==============================] - 97s 19ms/step - loss: 1.9157 - val_loss: 3.3835\n",
            "Epoch 7/100\n",
            "5162/5162 [==============================] - 97s 19ms/step - loss: 1.6174 - val_loss: 3.3434\n",
            "Epoch 8/100\n",
            "5162/5162 [==============================] - 97s 19ms/step - loss: 1.3927 - val_loss: 3.2500\n",
            "Epoch 9/100\n",
            "5162/5162 [==============================] - 97s 19ms/step - loss: 1.2389 - val_loss: 3.1910\n",
            "Epoch 10/100\n",
            "5162/5162 [==============================] - 97s 19ms/step - loss: 1.0849 - val_loss: 3.1342\n",
            "Epoch 11/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 0.9510 - val_loss: 3.0783\n",
            "Epoch 12/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 0.8581 - val_loss: 3.0721\n",
            "Epoch 13/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 0.7807 - val_loss: 3.0624\n",
            "Epoch 14/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 0.7075 - val_loss: 3.0783\n",
            "Epoch 15/100\n",
            "5162/5162 [==============================] - 97s 19ms/step - loss: 0.6440 - val_loss: 2.9966\n",
            "Epoch 16/100\n",
            "5162/5162 [==============================] - 97s 19ms/step - loss: 0.5988 - val_loss: 3.0342\n",
            "Epoch 17/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 0.5361 - val_loss: 2.9836\n",
            "Epoch 18/100\n",
            "5162/5162 [==============================] - 97s 19ms/step - loss: 0.4952 - val_loss: 2.9696\n",
            "Epoch 19/100\n",
            "5162/5162 [==============================] - 96s 19ms/step - loss: 0.4633 - val_loss: 3.0027\n",
            "Epoch 20/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 0.4289 - val_loss: 2.9858\n",
            "Epoch 21/100\n",
            "5162/5162 [==============================] - 97s 19ms/step - loss: 0.3950 - val_loss: 2.9081\n",
            "Epoch 22/100\n",
            "5162/5162 [==============================] - 97s 19ms/step - loss: 0.3773 - val_loss: 2.8838\n",
            "Epoch 23/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 0.3428 - val_loss: 2.9224\n",
            "Epoch 24/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 0.3274 - val_loss: 2.9301\n",
            "Epoch 25/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 0.3100 - val_loss: 2.8961\n",
            "Epoch 26/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 0.2897 - val_loss: 2.8532\n",
            "Epoch 27/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 0.2755 - val_loss: 2.8385\n",
            "Epoch 28/100\n",
            "5162/5162 [==============================] - 97s 19ms/step - loss: 0.2686 - val_loss: 2.8708\n",
            "Epoch 29/100\n",
            "5162/5162 [==============================] - 97s 19ms/step - loss: 0.2487 - val_loss: 2.8357\n",
            "Epoch 30/100\n",
            "5162/5162 [==============================] - 97s 19ms/step - loss: 0.2351 - val_loss: 2.8228\n",
            "Epoch 31/100\n",
            "5162/5162 [==============================] - 96s 19ms/step - loss: 0.2303 - val_loss: 2.8814\n",
            "Epoch 32/100\n",
            "5162/5162 [==============================] - 96s 19ms/step - loss: 0.2132 - val_loss: 2.8141\n",
            "Epoch 33/100\n",
            "5162/5162 [==============================] - 97s 19ms/step - loss: 0.2075 - val_loss: 2.7518\n",
            "Epoch 34/100\n",
            "5162/5162 [==============================] - 97s 19ms/step - loss: 0.1977 - val_loss: 2.8034\n",
            "Epoch 35/100\n",
            "5162/5162 [==============================] - 96s 19ms/step - loss: 0.1970 - val_loss: 2.8106\n",
            "Epoch 36/100\n",
            "5162/5162 [==============================] - 96s 19ms/step - loss: 0.1788 - val_loss: 2.7792\n",
            "Epoch 37/100\n",
            "5162/5162 [==============================] - 96s 19ms/step - loss: 0.1765 - val_loss: 2.7670\n",
            "Epoch 38/100\n",
            "5162/5162 [==============================] - 95s 18ms/step - loss: 0.1693 - val_loss: 2.8235\n",
            " finished embed deep=True, mse: 2.74\n",
            "iteration 0, deep=True, mse change from mse_lm: -2.93%\n",
            "iteration 1\n",
            "Epoch 1/100\n",
            "5162/5162 [==============================] - 93s 18ms/step - loss: 10.2133 - val_loss: 4.8907\n",
            "Epoch 2/100\n",
            "5162/5162 [==============================] - 91s 18ms/step - loss: 4.2511 - val_loss: 4.2080\n",
            "Epoch 3/100\n",
            "5162/5162 [==============================] - 91s 18ms/step - loss: 3.2930 - val_loss: 3.9152\n",
            "Epoch 4/100\n",
            "5162/5162 [==============================] - 90s 17ms/step - loss: 2.7433 - val_loss: 3.7953\n",
            "Epoch 5/100\n",
            "5162/5162 [==============================] - 90s 17ms/step - loss: 2.2693 - val_loss: 3.5207\n",
            "Epoch 6/100\n",
            "5162/5162 [==============================] - 90s 18ms/step - loss: 1.8987 - val_loss: 3.4871\n",
            "Epoch 7/100\n",
            "5162/5162 [==============================] - 91s 18ms/step - loss: 1.6328 - val_loss: 3.4922\n",
            "Epoch 8/100\n",
            "5162/5162 [==============================] - 91s 18ms/step - loss: 1.4107 - val_loss: 3.3268\n",
            "Epoch 9/100\n",
            "5162/5162 [==============================] - 90s 17ms/step - loss: 1.2399 - val_loss: 3.3483\n",
            "Epoch 10/100\n",
            "5162/5162 [==============================] - 90s 17ms/step - loss: 1.0718 - val_loss: 3.2229\n",
            "Epoch 11/100\n",
            "5162/5162 [==============================] - 90s 17ms/step - loss: 0.9559 - val_loss: 3.3582\n",
            "Epoch 12/100\n",
            "5162/5162 [==============================] - 90s 17ms/step - loss: 0.8483 - val_loss: 3.1072\n",
            "Epoch 13/100\n",
            "5162/5162 [==============================] - 90s 17ms/step - loss: 0.7421 - val_loss: 3.1169\n",
            "Epoch 14/100\n",
            "5162/5162 [==============================] - 90s 17ms/step - loss: 0.6684 - val_loss: 3.1020\n",
            "Epoch 15/100\n",
            "5162/5162 [==============================] - 90s 17ms/step - loss: 0.5912 - val_loss: 3.0614\n",
            "Epoch 16/100\n",
            "5162/5162 [==============================] - 89s 17ms/step - loss: 0.5485 - val_loss: 3.0599\n",
            "Epoch 17/100\n",
            "5162/5162 [==============================] - 89s 17ms/step - loss: 0.4947 - val_loss: 3.0634\n",
            "Epoch 18/100\n",
            "5162/5162 [==============================] - 90s 17ms/step - loss: 0.4648 - val_loss: 2.9509\n",
            "Epoch 19/100\n",
            "5162/5162 [==============================] - 89s 17ms/step - loss: 0.4167 - val_loss: 2.9489\n",
            "Epoch 20/100\n",
            "5162/5162 [==============================] - 89s 17ms/step - loss: 0.3842 - val_loss: 2.9670\n",
            "Epoch 21/100\n",
            "5162/5162 [==============================] - 89s 17ms/step - loss: 0.3529 - val_loss: 3.0036\n",
            "Epoch 22/100\n",
            "5162/5162 [==============================] - 89s 17ms/step - loss: 0.3258 - val_loss: 2.9873\n",
            "Epoch 23/100\n",
            "5162/5162 [==============================] - 90s 17ms/step - loss: 0.3079 - val_loss: 2.9407\n",
            "Epoch 24/100\n",
            "5162/5162 [==============================] - 90s 17ms/step - loss: 0.2898 - val_loss: 2.9163\n",
            "Epoch 25/100\n",
            "5162/5162 [==============================] - 89s 17ms/step - loss: 0.2755 - val_loss: 2.9142\n",
            "Epoch 26/100\n",
            "5162/5162 [==============================] - 89s 17ms/step - loss: 0.2602 - val_loss: 2.8675\n",
            "Epoch 27/100\n",
            "5162/5162 [==============================] - 90s 18ms/step - loss: 0.2546 - val_loss: 2.8683\n",
            "Epoch 28/100\n",
            "5162/5162 [==============================] - 89s 17ms/step - loss: 0.2333 - val_loss: 2.8466\n",
            "Epoch 29/100\n",
            "5162/5162 [==============================] - 89s 17ms/step - loss: 0.2201 - val_loss: 2.8819\n",
            "Epoch 30/100\n",
            "5162/5162 [==============================] - 89s 17ms/step - loss: 0.2055 - val_loss: 2.8520\n",
            "Epoch 31/100\n",
            "5162/5162 [==============================] - 89s 17ms/step - loss: 0.2040 - val_loss: 2.8439\n",
            "Epoch 32/100\n",
            "5162/5162 [==============================] - 89s 17ms/step - loss: 0.1850 - val_loss: 2.9230\n",
            "Epoch 33/100\n",
            "5162/5162 [==============================] - 90s 17ms/step - loss: 0.1895 - val_loss: 2.8624\n",
            "Epoch 34/100\n",
            "5162/5162 [==============================] - 90s 17ms/step - loss: 0.1691 - val_loss: 2.8909\n",
            "Epoch 35/100\n",
            "5162/5162 [==============================] - 89s 17ms/step - loss: 0.1802 - val_loss: 2.8438\n",
            "Epoch 36/100\n",
            "5162/5162 [==============================] - 90s 17ms/step - loss: 0.1547 - val_loss: 2.8474\n",
            "Epoch 37/100\n",
            "5162/5162 [==============================] - 90s 17ms/step - loss: 0.1564 - val_loss: 2.8510\n",
            "Epoch 38/100\n",
            "5162/5162 [==============================] - 89s 17ms/step - loss: 0.1537 - val_loss: 2.8849\n",
            "Epoch 39/100\n",
            "5162/5162 [==============================] - 89s 17ms/step - loss: 0.1419 - val_loss: 2.8177\n",
            "Epoch 40/100\n",
            "5162/5162 [==============================] - 90s 17ms/step - loss: 0.1414 - val_loss: 2.7848\n",
            "Epoch 41/100\n",
            "5162/5162 [==============================] - 91s 18ms/step - loss: 0.1362 - val_loss: 2.8162\n",
            "Epoch 42/100\n",
            "5162/5162 [==============================] - 90s 17ms/step - loss: 0.1315 - val_loss: 2.8110\n",
            "Epoch 43/100\n",
            "5162/5162 [==============================] - 90s 18ms/step - loss: 0.1259 - val_loss: 2.8030\n",
            "Epoch 44/100\n",
            "5162/5162 [==============================] - 91s 18ms/step - loss: 0.1284 - val_loss: 2.7876\n",
            "Epoch 45/100\n",
            "5162/5162 [==============================] - 90s 17ms/step - loss: 0.1206 - val_loss: 2.8017\n",
            " finished ignore deep=True, mse: 2.75\n",
            "Epoch 1/100\n",
            "5162/5162 [==============================] - 102s 19ms/step - loss: 102.1419 - val_loss: 66.6208\n",
            "Epoch 2/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 64.4067 - val_loss: 64.6098\n",
            "Epoch 3/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 61.1960 - val_loss: 63.3477\n",
            "Epoch 4/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 57.9727 - val_loss: 63.1245\n",
            "Epoch 5/100\n",
            "5162/5162 [==============================] - 100s 19ms/step - loss: 55.2068 - val_loss: 62.8674\n",
            "Epoch 6/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 52.3687 - val_loss: 64.1689\n",
            "Epoch 7/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 49.9179 - val_loss: 64.7874\n",
            "Epoch 8/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 47.8178 - val_loss: 65.9781\n",
            "Epoch 9/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 45.6692 - val_loss: 68.2445\n",
            "Epoch 10/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 43.7252 - val_loss: 70.0869\n",
            "Epoch 11/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 41.9065 - val_loss: 72.8758\n",
            " sig2e: 1.02, sig2b: 0.05\n",
            "Epoch 12/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 40.2191 - val_loss: 79.8763\n",
            "Epoch 13/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 38.8691 - val_loss: 83.1467\n",
            "Epoch 14/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 37.1424 - val_loss: 83.5207\n",
            "Epoch 15/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 35.6385 - val_loss: 87.3559\n",
            "Epoch 16/100\n",
            "5162/5162 [==============================] - 97s 19ms/step - loss: 34.1303 - val_loss: 90.4154\n",
            "Epoch 17/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 32.7108 - val_loss: 96.1587\n",
            "Epoch 18/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 31.6576 - val_loss: 105.0562\n",
            "Epoch 19/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 30.1797 - val_loss: 111.1463\n",
            "Epoch 20/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 29.4297 - val_loss: 118.7142\n",
            "Epoch 21/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 28.1518 - val_loss: 112.7289\n",
            " sig2e: 0.42, sig2b: 0.03\n",
            "Epoch 22/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 26.8603 - val_loss: 116.6576\n",
            "Epoch 23/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 26.1185 - val_loss: 128.6150\n",
            "Epoch 24/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 25.3850 - val_loss: 141.5738\n",
            "Epoch 25/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 24.3324 - val_loss: 144.2868\n",
            "Epoch 26/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 23.5300 - val_loss: 142.0886\n",
            "Epoch 27/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 23.0247 - val_loss: 151.9868\n",
            "Epoch 28/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 22.1063 - val_loss: 172.1169\n",
            "Epoch 29/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 21.1958 - val_loss: 144.4841\n",
            "Epoch 30/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 20.5580 - val_loss: 178.1914\n",
            "Epoch 31/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 19.7209 - val_loss: 180.7099\n",
            " sig2e: 0.23, sig2b: 0.01\n",
            "Epoch 32/100\n",
            "5162/5162 [==============================] - 99s 19ms/step - loss: 19.0389 - val_loss: 176.8455\n",
            "Epoch 33/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 18.5190 - val_loss: 194.4764\n",
            "Epoch 34/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 18.3363 - val_loss: 204.5925\n",
            "Epoch 35/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 17.4188 - val_loss: 212.5802\n",
            "Epoch 36/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 16.8998 - val_loss: 211.4832\n",
            "Epoch 37/100\n",
            "5162/5162 [==============================] - 97s 19ms/step - loss: 16.5921 - val_loss: 235.1161\n",
            "Epoch 38/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 15.8541 - val_loss: 215.6487\n",
            "Epoch 39/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 15.2505 - val_loss: 218.8705\n",
            "Epoch 40/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 15.1268 - val_loss: 253.6908\n",
            "Epoch 41/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 14.4571 - val_loss: 236.2533\n",
            " sig2e: 0.16, sig2b: 0.02\n",
            "Epoch 42/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 14.1151 - val_loss: 248.8648\n",
            "Epoch 43/100\n",
            "5162/5162 [==============================] - 99s 19ms/step - loss: 13.9107 - val_loss: 288.0941\n",
            "Epoch 44/100\n",
            "5162/5162 [==============================] - 100s 19ms/step - loss: 13.0840 - val_loss: 276.2170\n",
            "Epoch 45/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 13.0851 - val_loss: 273.4894\n",
            "Epoch 46/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 12.2961 - val_loss: 275.7201\n",
            "Epoch 47/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 12.1281 - val_loss: 320.5540\n",
            "Epoch 48/100\n",
            "5162/5162 [==============================] - 99s 19ms/step - loss: 12.0832 - val_loss: 277.5785\n",
            "Epoch 49/100\n",
            "5162/5162 [==============================] - 99s 19ms/step - loss: 11.4538 - val_loss: 327.7562\n",
            "Epoch 50/100\n",
            "5162/5162 [==============================] - 99s 19ms/step - loss: 12.0109 - val_loss: 306.4485\n",
            "Epoch 51/100\n",
            "5162/5162 [==============================] - 99s 19ms/step - loss: 10.7227 - val_loss: 280.1286\n",
            " sig2e: 0.13, sig2b: 0.02\n",
            "Epoch 52/100\n",
            "5162/5162 [==============================] - 99s 19ms/step - loss: 11.0990 - val_loss: 295.9091\n",
            "Epoch 53/100\n",
            "5162/5162 [==============================] - 99s 19ms/step - loss: 9.9850 - val_loss: 313.1395\n",
            "Epoch 54/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 9.9313 - val_loss: 312.5920\n",
            "Epoch 55/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 9.9255 - val_loss: 289.2929\n",
            "Epoch 56/100\n",
            "5162/5162 [==============================] - 99s 19ms/step - loss: 10.6420 - val_loss: 349.2708\n",
            "Epoch 57/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 9.4999 - val_loss: 395.8224\n",
            "Epoch 58/100\n",
            "5162/5162 [==============================] - 100s 19ms/step - loss: 8.8533 - val_loss: 381.6367\n",
            "Epoch 59/100\n",
            "5162/5162 [==============================] - 100s 19ms/step - loss: 9.0901 - val_loss: 402.5976\n",
            "Epoch 60/100\n",
            "5162/5162 [==============================] - 100s 19ms/step - loss: 8.2748 - val_loss: 391.8589\n",
            "Epoch 61/100\n",
            "5162/5162 [==============================] - 100s 19ms/step - loss: 7.9803 - val_loss: 353.9287\n",
            " sig2e: 0.10, sig2b: 0.01\n",
            "Epoch 62/100\n",
            "5162/5162 [==============================] - 100s 19ms/step - loss: 9.4678 - val_loss: 347.3124\n",
            "Epoch 63/100\n",
            "5162/5162 [==============================] - 100s 19ms/step - loss: 7.6285 - val_loss: 383.3789\n",
            "Epoch 64/100\n",
            "5162/5162 [==============================] - 100s 19ms/step - loss: 7.4040 - val_loss: 321.0498\n",
            "Epoch 65/100\n",
            "5162/5162 [==============================] - 100s 19ms/step - loss: 7.6470 - val_loss: 396.7277\n",
            "Epoch 66/100\n",
            "5162/5162 [==============================] - 100s 19ms/step - loss: 7.3092 - val_loss: 354.4474\n",
            "Epoch 67/100\n",
            "5162/5162 [==============================] - 100s 19ms/step - loss: 7.1587 - val_loss: 351.4931\n",
            "Epoch 68/100\n",
            "5162/5162 [==============================] - 101s 20ms/step - loss: 6.6564 - val_loss: 344.2124\n",
            "Epoch 69/100\n",
            "5162/5162 [==============================] - 100s 19ms/step - loss: 6.6121 - val_loss: 343.1120\n",
            "Epoch 70/100\n",
            "5162/5162 [==============================] - 100s 19ms/step - loss: 7.0880 - val_loss: 309.4984\n",
            " finished lmm deep=True, mse: 2.67\n",
            "Epoch 1/100\n",
            "5162/5162 [==============================] - 94s 18ms/step - loss: 11.5370 - val_loss: 4.9926\n",
            "Epoch 2/100\n",
            "5162/5162 [==============================] - 91s 18ms/step - loss: 4.4739 - val_loss: 4.2560\n",
            "Epoch 3/100\n",
            "5162/5162 [==============================] - 91s 18ms/step - loss: 3.5588 - val_loss: 3.9768\n",
            "Epoch 4/100\n",
            "5162/5162 [==============================] - 93s 18ms/step - loss: 2.9093 - val_loss: 3.7297\n",
            "Epoch 5/100\n",
            "5162/5162 [==============================] - 93s 18ms/step - loss: 2.4135 - val_loss: 3.6988\n",
            "Epoch 6/100\n",
            "5162/5162 [==============================] - 92s 18ms/step - loss: 2.0503 - val_loss: 3.5342\n",
            "Epoch 7/100\n",
            "5162/5162 [==============================] - 92s 18ms/step - loss: 1.7666 - val_loss: 3.3924\n",
            "Epoch 8/100\n",
            "5162/5162 [==============================] - 92s 18ms/step - loss: 1.4934 - val_loss: 3.3348\n",
            "Epoch 9/100\n",
            "5162/5162 [==============================] - 92s 18ms/step - loss: 1.3109 - val_loss: 3.2751\n",
            "Epoch 10/100\n",
            "5162/5162 [==============================] - 92s 18ms/step - loss: 1.1638 - val_loss: 3.1807\n",
            "Epoch 11/100\n",
            "5162/5162 [==============================] - 91s 18ms/step - loss: 1.0097 - val_loss: 3.2186\n",
            "Epoch 12/100\n",
            "5162/5162 [==============================] - 91s 18ms/step - loss: 0.8973 - val_loss: 3.1448\n",
            "Epoch 13/100\n",
            "5162/5162 [==============================] - 95s 18ms/step - loss: 0.7974 - val_loss: 3.1694\n",
            "Epoch 14/100\n",
            "5162/5162 [==============================] - 92s 18ms/step - loss: 0.7191 - val_loss: 3.0786\n",
            "Epoch 15/100\n",
            "5162/5162 [==============================] - 92s 18ms/step - loss: 0.6389 - val_loss: 3.1550\n",
            "Epoch 16/100\n",
            "5162/5162 [==============================] - 94s 18ms/step - loss: 0.5981 - val_loss: 3.0983\n",
            "Epoch 17/100\n",
            "5162/5162 [==============================] - 93s 18ms/step - loss: 0.5444 - val_loss: 3.0265\n",
            "Epoch 18/100\n",
            "5162/5162 [==============================] - 91s 18ms/step - loss: 0.5006 - val_loss: 3.1738\n",
            "Epoch 19/100\n",
            "5162/5162 [==============================] - 91s 18ms/step - loss: 0.4604 - val_loss: 3.0432\n",
            "Epoch 20/100\n",
            "5162/5162 [==============================] - 91s 18ms/step - loss: 0.4217 - val_loss: 2.9796\n",
            "Epoch 21/100\n",
            "5162/5162 [==============================] - 92s 18ms/step - loss: 0.3899 - val_loss: 2.9845\n",
            "Epoch 22/100\n",
            "5162/5162 [==============================] - 96s 19ms/step - loss: 0.3762 - val_loss: 2.9675\n",
            "Epoch 23/100\n",
            "5162/5162 [==============================] - 91s 18ms/step - loss: 0.3452 - val_loss: 2.9633\n",
            "Epoch 24/100\n",
            "5162/5162 [==============================] - 91s 18ms/step - loss: 0.3359 - val_loss: 3.0383\n",
            "Epoch 25/100\n",
            "5162/5162 [==============================] - 91s 18ms/step - loss: 0.3071 - val_loss: 2.9412\n",
            "Epoch 26/100\n",
            "5162/5162 [==============================] - 91s 18ms/step - loss: 0.2906 - val_loss: 2.9459\n",
            "Epoch 27/100\n",
            "5162/5162 [==============================] - 91s 18ms/step - loss: 0.2750 - val_loss: 2.9296\n",
            "Epoch 28/100\n",
            "5162/5162 [==============================] - 90s 18ms/step - loss: 0.2642 - val_loss: 2.9345\n",
            "Epoch 29/100\n",
            "5162/5162 [==============================] - 92s 18ms/step - loss: 0.2533 - val_loss: 2.8960\n",
            "Epoch 30/100\n",
            "5162/5162 [==============================] - 92s 18ms/step - loss: 0.2340 - val_loss: 2.9073\n",
            "Epoch 31/100\n",
            "5162/5162 [==============================] - 92s 18ms/step - loss: 0.2276 - val_loss: 2.9218\n",
            "Epoch 32/100\n",
            "5162/5162 [==============================] - 92s 18ms/step - loss: 0.2205 - val_loss: 2.9012\n",
            "Epoch 33/100\n",
            "5162/5162 [==============================] - 92s 18ms/step - loss: 0.2068 - val_loss: 2.9482\n",
            "Epoch 34/100\n",
            "5162/5162 [==============================] - 92s 18ms/step - loss: 0.2132 - val_loss: 2.9059\n",
            " finished lm deep=True, mse: 2.78\n",
            "Epoch 1/100\n",
            "5162/5162 [==============================] - 99s 19ms/step - loss: 9.7491 - val_loss: 4.6912\n",
            "Epoch 2/100\n",
            "5162/5162 [==============================] - 97s 19ms/step - loss: 4.0836 - val_loss: 4.2194\n",
            "Epoch 3/100\n",
            "5162/5162 [==============================] - 97s 19ms/step - loss: 3.1851 - val_loss: 3.7855\n",
            "Epoch 4/100\n",
            "5162/5162 [==============================] - 97s 19ms/step - loss: 2.6414 - val_loss: 3.6108\n",
            "Epoch 5/100\n",
            "5162/5162 [==============================] - 97s 19ms/step - loss: 2.1902 - val_loss: 3.4580\n",
            "Epoch 6/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 1.8453 - val_loss: 3.4135\n",
            "Epoch 7/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 1.6120 - val_loss: 3.3239\n",
            "Epoch 8/100\n",
            "5162/5162 [==============================] - 97s 19ms/step - loss: 1.4001 - val_loss: 3.3520\n",
            "Epoch 9/100\n",
            "5162/5162 [==============================] - 97s 19ms/step - loss: 1.2404 - val_loss: 3.2990\n",
            "Epoch 10/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 1.1009 - val_loss: 3.2825\n",
            "Epoch 11/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 0.9892 - val_loss: 3.2956\n",
            "Epoch 12/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 0.8984 - val_loss: 3.1755\n",
            "Epoch 13/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 0.8077 - val_loss: 3.1713\n",
            "Epoch 14/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 0.7394 - val_loss: 3.2754\n",
            "Epoch 15/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 0.6813 - val_loss: 3.0921\n",
            "Epoch 16/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 0.6241 - val_loss: 3.1142\n",
            "Epoch 17/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 0.5783 - val_loss: 3.0710\n",
            "Epoch 18/100\n",
            "5162/5162 [==============================] - 99s 19ms/step - loss: 0.5277 - val_loss: 3.0771\n",
            "Epoch 19/100\n",
            "5162/5162 [==============================] - 100s 19ms/step - loss: 0.4889 - val_loss: 3.0664\n",
            "Epoch 20/100\n",
            "5162/5162 [==============================] - 99s 19ms/step - loss: 0.4539 - val_loss: 3.1007\n",
            "Epoch 21/100\n",
            "5162/5162 [==============================] - 99s 19ms/step - loss: 0.4273 - val_loss: 3.0999\n",
            "Epoch 22/100\n",
            "5162/5162 [==============================] - 99s 19ms/step - loss: 0.3892 - val_loss: 3.0403\n",
            "Epoch 23/100\n",
            "5162/5162 [==============================] - 99s 19ms/step - loss: 0.3618 - val_loss: 3.0475\n",
            "Epoch 24/100\n",
            "5162/5162 [==============================] - 99s 19ms/step - loss: 0.3428 - val_loss: 2.9525\n",
            "Epoch 25/100\n",
            "5162/5162 [==============================] - 101s 20ms/step - loss: 0.3207 - val_loss: 3.1366\n",
            "Epoch 26/100\n",
            "5162/5162 [==============================] - 99s 19ms/step - loss: 0.3071 - val_loss: 3.0025\n",
            "Epoch 27/100\n",
            "5162/5162 [==============================] - 99s 19ms/step - loss: 0.2866 - val_loss: 3.1506\n",
            "Epoch 28/100\n",
            "5162/5162 [==============================] - 99s 19ms/step - loss: 0.3101 - val_loss: 3.0388\n",
            "Epoch 29/100\n",
            "5162/5162 [==============================] - 99s 19ms/step - loss: 0.2572 - val_loss: 3.0101\n",
            " finished embed deep=True, mse: 2.91\n",
            "iteration 1, deep=True, mse change from mse_lm: -3.65%\n",
            "iteration 2\n",
            "Epoch 1/100\n",
            "5162/5162 [==============================] - 96s 18ms/step - loss: 9.7910 - val_loss: 4.6252\n",
            "Epoch 2/100\n",
            "5162/5162 [==============================] - 92s 18ms/step - loss: 4.1446 - val_loss: 4.0506\n",
            "Epoch 3/100\n",
            "5162/5162 [==============================] - 93s 18ms/step - loss: 3.2998 - val_loss: 3.8464\n",
            "Epoch 4/100\n",
            "5162/5162 [==============================] - 93s 18ms/step - loss: 2.6991 - val_loss: 3.5911\n",
            "Epoch 5/100\n",
            "5162/5162 [==============================] - 93s 18ms/step - loss: 2.2627 - val_loss: 3.5006\n",
            "Epoch 6/100\n",
            "5162/5162 [==============================] - 93s 18ms/step - loss: 1.9243 - val_loss: 3.4355\n",
            "Epoch 7/100\n",
            "5162/5162 [==============================] - 93s 18ms/step - loss: 1.6596 - val_loss: 3.2983\n",
            "Epoch 8/100\n",
            "5162/5162 [==============================] - 93s 18ms/step - loss: 1.4711 - val_loss: 3.3482\n",
            "Epoch 9/100\n",
            "5162/5162 [==============================] - 94s 18ms/step - loss: 1.2582 - val_loss: 3.2027\n",
            "Epoch 10/100\n",
            "5162/5162 [==============================] - 92s 18ms/step - loss: 1.1421 - val_loss: 3.1229\n",
            "Epoch 11/100\n",
            "5162/5162 [==============================] - 93s 18ms/step - loss: 1.0050 - val_loss: 3.0703\n",
            "Epoch 12/100\n",
            "5162/5162 [==============================] - 93s 18ms/step - loss: 0.8892 - val_loss: 3.1178\n",
            "Epoch 13/100\n",
            "5162/5162 [==============================] - 93s 18ms/step - loss: 0.8110 - val_loss: 3.0514\n",
            "Epoch 14/100\n",
            "5162/5162 [==============================] - 93s 18ms/step - loss: 0.7124 - val_loss: 2.9896\n",
            "Epoch 15/100\n",
            "5162/5162 [==============================] - 93s 18ms/step - loss: 0.6643 - val_loss: 2.9444\n",
            "Epoch 16/100\n",
            "5162/5162 [==============================] - 93s 18ms/step - loss: 0.5933 - val_loss: 2.9167\n",
            "Epoch 17/100\n",
            "5162/5162 [==============================] - 93s 18ms/step - loss: 0.5470 - val_loss: 2.9160\n",
            "Epoch 18/100\n",
            "5162/5162 [==============================] - 92s 18ms/step - loss: 0.4895 - val_loss: 2.8380\n",
            "Epoch 19/100\n",
            "5162/5162 [==============================] - 93s 18ms/step - loss: 0.4599 - val_loss: 2.8566\n",
            "Epoch 20/100\n",
            "5162/5162 [==============================] - 93s 18ms/step - loss: 0.4286 - val_loss: 2.8490\n",
            "Epoch 21/100\n",
            "5162/5162 [==============================] - 94s 18ms/step - loss: 0.3941 - val_loss: 2.8206\n",
            "Epoch 22/100\n",
            "5162/5162 [==============================] - 96s 19ms/step - loss: 0.3657 - val_loss: 2.8066\n",
            "Epoch 23/100\n",
            "5162/5162 [==============================] - 96s 19ms/step - loss: 0.3446 - val_loss: 2.8845\n",
            "Epoch 24/100\n",
            "5162/5162 [==============================] - 97s 19ms/step - loss: 0.3258 - val_loss: 2.8026\n",
            "Epoch 25/100\n",
            "5162/5162 [==============================] - 95s 18ms/step - loss: 0.2996 - val_loss: 2.7948\n",
            "Epoch 26/100\n",
            "5162/5162 [==============================] - 96s 19ms/step - loss: 0.2852 - val_loss: 2.7714\n",
            "Epoch 27/100\n",
            "5162/5162 [==============================] - 96s 19ms/step - loss: 0.2777 - val_loss: 2.7497\n",
            "Epoch 28/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 0.2497 - val_loss: 2.7955\n",
            "Epoch 29/100\n",
            "5162/5162 [==============================] - 97s 19ms/step - loss: 0.2479 - val_loss: 2.7342\n",
            "Epoch 30/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 0.2299 - val_loss: 2.7237\n",
            "Epoch 31/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 0.2210 - val_loss: 2.7212\n",
            "Epoch 32/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 0.2104 - val_loss: 2.7249\n",
            "Epoch 33/100\n",
            "5162/5162 [==============================] - 99s 19ms/step - loss: 0.2046 - val_loss: 2.7333\n",
            "Epoch 34/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 0.1963 - val_loss: 2.6964\n",
            "Epoch 35/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 0.1928 - val_loss: 2.7109\n",
            "Epoch 36/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 0.1823 - val_loss: 2.6603\n",
            "Epoch 37/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 0.1761 - val_loss: 2.7239\n",
            "Epoch 38/100\n",
            "5162/5162 [==============================] - 97s 19ms/step - loss: 0.1729 - val_loss: 2.6951\n",
            "Epoch 39/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 0.1617 - val_loss: 2.7204\n",
            "Epoch 40/100\n",
            "5162/5162 [==============================] - 97s 19ms/step - loss: 0.1589 - val_loss: 2.7225\n",
            "Epoch 41/100\n",
            "5162/5162 [==============================] - 98s 19ms/step - loss: 0.1487 - val_loss: 2.7211\n",
            " finished ignore deep=True, mse: 2.63\n",
            "Epoch 1/100\n",
            "5162/5162 [==============================] - 109s 21ms/step - loss: 100.1415 - val_loss: 66.0640\n",
            "Epoch 2/100\n",
            "5162/5162 [==============================] - 104s 20ms/step - loss: 64.6661 - val_loss: 64.2915\n",
            "Epoch 3/100\n",
            "5162/5162 [==============================] - 105s 20ms/step - loss: 61.0486 - val_loss: 62.7269\n",
            "Epoch 4/100\n",
            "5162/5162 [==============================] - 106s 20ms/step - loss: 57.8545 - val_loss: 62.8151\n",
            "Epoch 5/100\n",
            "5162/5162 [==============================] - 105s 20ms/step - loss: 55.2196 - val_loss: 63.1976\n",
            "Epoch 6/100\n",
            "5162/5162 [==============================] - 105s 20ms/step - loss: 52.6170 - val_loss: 63.5858\n",
            "Epoch 7/100\n",
            "5162/5162 [==============================] - 105s 20ms/step - loss: 50.5666 - val_loss: 64.4256\n",
            "Epoch 8/100\n",
            "5162/5162 [==============================] - 106s 20ms/step - loss: 48.2970 - val_loss: 66.5328\n",
            "Epoch 9/100\n",
            "5162/5162 [==============================] - 105s 20ms/step - loss: 46.0492 - val_loss: 67.2405\n",
            "Epoch 10/100\n",
            "5162/5162 [==============================] - 106s 20ms/step - loss: 44.3290 - val_loss: 68.5480\n",
            "Epoch 11/100\n",
            "5162/5162 [==============================] - 106s 20ms/step - loss: 42.2671 - val_loss: 71.7507\n",
            " sig2e: 1.05, sig2b: 0.06\n",
            "Epoch 12/100\n",
            "5162/5162 [==============================] - 106s 20ms/step - loss: 40.9377 - val_loss: 74.7258\n",
            "Epoch 13/100\n",
            "5162/5162 [==============================] - 106s 21ms/step - loss: 39.1939 - val_loss: 77.7263\n",
            "Epoch 14/100\n",
            "5162/5162 [==============================] - 106s 21ms/step - loss: 37.6326 - val_loss: 83.9387\n",
            "Epoch 15/100\n",
            "5162/5162 [==============================] - 108s 21ms/step - loss: 36.2520 - val_loss: 87.1546\n",
            "Epoch 16/100\n",
            "5162/5162 [==============================] - 106s 21ms/step - loss: 34.7333 - val_loss: 89.9576\n",
            "Epoch 17/100\n",
            "5162/5162 [==============================] - 106s 20ms/step - loss: 33.6194 - val_loss: 95.9357\n",
            "Epoch 18/100\n",
            "5162/5162 [==============================] - 106s 21ms/step - loss: 32.1460 - val_loss: 99.0657\n",
            "Epoch 19/100\n",
            "5162/5162 [==============================] - 106s 21ms/step - loss: 31.0402 - val_loss: 101.3731\n",
            "Epoch 20/100\n",
            "5162/5162 [==============================] - 106s 20ms/step - loss: 29.6315 - val_loss: 106.0076\n",
            "Epoch 21/100\n",
            "5162/5162 [==============================] - 105s 20ms/step - loss: 28.6943 - val_loss: 110.2152\n",
            " sig2e: 0.42, sig2b: 0.03\n",
            "Epoch 22/100\n",
            "5162/5162 [==============================] - 106s 21ms/step - loss: 27.6103 - val_loss: 120.2838\n",
            "Epoch 23/100\n",
            "5162/5162 [==============================] - 105s 20ms/step - loss: 26.6673 - val_loss: 124.6239\n",
            "Epoch 24/100\n",
            "5162/5162 [==============================] - 106s 21ms/step - loss: 25.6660 - val_loss: 131.9052\n",
            "Epoch 25/100\n",
            "5162/5162 [==============================] - 105s 20ms/step - loss: 24.5294 - val_loss: 139.3201\n",
            "Epoch 26/100\n",
            "5162/5162 [==============================] - 103s 20ms/step - loss: 23.9495 - val_loss: 141.8443\n",
            "Epoch 27/100\n",
            "5162/5162 [==============================] - 103s 20ms/step - loss: 22.5697 - val_loss: 153.2410\n",
            "Epoch 28/100\n",
            "5162/5162 [==============================] - 103s 20ms/step - loss: 22.1895 - val_loss: 164.2841\n",
            "Epoch 29/100\n",
            "5162/5162 [==============================] - 103s 20ms/step - loss: 21.2086 - val_loss: 151.0439\n",
            "Epoch 30/100\n",
            "5162/5162 [==============================] - 104s 20ms/step - loss: 20.8479 - val_loss: 172.9723\n",
            "Epoch 31/100\n",
            "5162/5162 [==============================] - 103s 20ms/step - loss: 19.5597 - val_loss: 163.6191\n",
            " sig2e: 0.24, sig2b: 0.02\n",
            "Epoch 32/100\n",
            "5162/5162 [==============================] - 103s 20ms/step - loss: 19.2393 - val_loss: 182.3084\n",
            "Epoch 33/100\n",
            "5162/5162 [==============================] - 103s 20ms/step - loss: 19.0114 - val_loss: 192.2768\n",
            "Epoch 34/100\n",
            "5162/5162 [==============================] - 103s 20ms/step - loss: 17.8596 - val_loss: 185.5212\n",
            "Epoch 35/100\n",
            "5162/5162 [==============================] - 103s 20ms/step - loss: 17.1015 - val_loss: 221.7824\n",
            "Epoch 36/100\n",
            "5162/5162 [==============================] - 103s 20ms/step - loss: 16.5302 - val_loss: 208.5728\n",
            "Epoch 37/100\n",
            "5162/5162 [==============================] - 103s 20ms/step - loss: 16.0596 - val_loss: 225.4268\n",
            "Epoch 38/100\n",
            "5162/5162 [==============================] - 103s 20ms/step - loss: 15.5929 - val_loss: 225.2044\n",
            "Epoch 39/100\n",
            "5162/5162 [==============================] - 103s 20ms/step - loss: 15.4389 - val_loss: 214.0442\n",
            "Epoch 40/100\n",
            "5162/5162 [==============================] - 104s 20ms/step - loss: 14.9948 - val_loss: 262.5561\n",
            "Epoch 41/100\n",
            "5162/5162 [==============================] - 104s 20ms/step - loss: 14.1016 - val_loss: 230.5241\n",
            " sig2e: 0.16, sig2b: 0.02\n",
            "Epoch 42/100\n",
            "5162/5162 [==============================] - 103s 20ms/step - loss: 13.6608 - val_loss: 244.8641\n",
            "Epoch 43/100\n",
            "5162/5162 [==============================] - 103s 20ms/step - loss: 13.3231 - val_loss: 277.5364\n",
            "Epoch 44/100\n",
            "5162/5162 [==============================] - 103s 20ms/step - loss: 13.2856 - val_loss: 305.2859\n",
            "Epoch 45/100\n",
            "5162/5162 [==============================] - 103s 20ms/step - loss: 12.6885 - val_loss: 263.5610\n",
            "Epoch 46/100\n",
            "5162/5162 [==============================] - 103s 20ms/step - loss: 12.6751 - val_loss: 277.8761\n",
            "Epoch 47/100\n",
            "5162/5162 [==============================] - 103s 20ms/step - loss: 11.7919 - val_loss: 258.1888\n",
            "Epoch 48/100\n",
            "5162/5162 [==============================] - 104s 20ms/step - loss: 11.2277 - val_loss: 264.9707\n",
            "Epoch 49/100\n",
            "5162/5162 [==============================] - 104s 20ms/step - loss: 11.1260 - val_loss: 319.4271\n",
            "Epoch 50/100\n",
            "5162/5162 [==============================] - 104s 20ms/step - loss: 10.5952 - val_loss: 323.0454\n",
            "Epoch 51/100\n",
            "5162/5162 [==============================] - 105s 20ms/step - loss: 10.4980 - val_loss: 274.9284\n",
            " sig2e: 0.13, sig2b: 0.01\n",
            "Epoch 52/100\n",
            "5162/5162 [==============================] - 104s 20ms/step - loss: 9.9433 - val_loss: 298.2980\n",
            "Epoch 53/100\n",
            "5162/5162 [==============================] - 103s 20ms/step - loss: 10.2432 - val_loss: 358.3732\n",
            "Epoch 54/100\n",
            "5162/5162 [==============================] - 103s 20ms/step - loss: 10.0987 - val_loss: 337.1552\n",
            "Epoch 55/100\n",
            "5162/5162 [==============================] - 103s 20ms/step - loss: 9.0621 - val_loss: 288.2495\n",
            "Epoch 56/100\n",
            "5162/5162 [==============================] - 102s 20ms/step - loss: 9.4390 - val_loss: 297.6777\n",
            "Epoch 57/100\n",
            "5162/5162 [==============================] - 103s 20ms/step - loss: 9.2934 - val_loss: 340.0014\n",
            "Epoch 58/100\n",
            "5162/5162 [==============================] - 103s 20ms/step - loss: 8.4462 - val_loss: 309.2372\n",
            "Epoch 59/100\n",
            "5162/5162 [==============================] - 102s 20ms/step - loss: 7.9497 - val_loss: 334.9177\n",
            "Epoch 60/100\n",
            "5162/5162 [==============================] - 103s 20ms/step - loss: 8.3828 - val_loss: 361.5603\n",
            "Epoch 61/100\n",
            "5162/5162 [==============================] - 103s 20ms/step - loss: 8.2918 - val_loss: 342.9725\n",
            " sig2e: 0.10, sig2b: 0.01\n",
            "Epoch 62/100\n",
            "5162/5162 [==============================] - 103s 20ms/step - loss: 7.7436 - val_loss: 328.1042\n",
            "Epoch 63/100\n",
            "5162/5162 [==============================] - 107s 21ms/step - loss: 7.6349 - val_loss: 373.1617\n",
            "Epoch 64/100\n",
            "5162/5162 [==============================] - 106s 21ms/step - loss: 6.5085 - val_loss: 392.0022\n",
            "Epoch 65/100\n",
            "5162/5162 [==============================] - 106s 21ms/step - loss: 6.8443 - val_loss: 389.8659\n",
            " finished lmm deep=True, mse: 2.65\n",
            "Epoch 1/100\n",
            "5162/5162 [==============================] - 102s 19ms/step - loss: 11.6829 - val_loss: 5.3747\n",
            "Epoch 2/100\n",
            "5162/5162 [==============================] - 99s 19ms/step - loss: 4.6930 - val_loss: 4.3614\n",
            "Epoch 3/100\n",
            "5162/5162 [==============================] - 99s 19ms/step - loss: 3.7075 - val_loss: 4.0763\n",
            "Epoch 4/100\n",
            "5162/5162 [==============================] - 100s 19ms/step - loss: 3.0775 - val_loss: 3.8600\n",
            "Epoch 5/100\n",
            "5162/5162 [==============================] - 100s 19ms/step - loss: 2.5524 - val_loss: 3.6148\n",
            "Epoch 6/100\n",
            "5162/5162 [==============================] - 101s 19ms/step - loss: 2.1673 - val_loss: 3.6016\n",
            "Epoch 7/100\n",
            "5162/5162 [==============================] - 100s 19ms/step - loss: 1.8879 - val_loss: 3.4265\n",
            "Epoch 8/100\n",
            "5162/5162 [==============================] - 100s 19ms/step - loss: 1.6456 - val_loss: 3.2713\n",
            "Epoch 9/100\n",
            "5162/5162 [==============================] - 100s 19ms/step - loss: 1.4419 - val_loss: 3.2483\n",
            "Epoch 10/100\n",
            "5162/5162 [==============================] - 100s 19ms/step - loss: 1.2626 - val_loss: 3.1373\n",
            "Epoch 11/100\n",
            "5162/5162 [==============================] - 100s 19ms/step - loss: 1.1317 - val_loss: 3.2107\n",
            "Epoch 12/100\n",
            "5162/5162 [==============================] - 99s 19ms/step - loss: 1.0067 - val_loss: 3.1315\n",
            "Epoch 13/100\n",
            "5162/5162 [==============================] - 99s 19ms/step - loss: 0.9167 - val_loss: 3.0149\n",
            "Epoch 14/100\n",
            "5162/5162 [==============================] - 99s 19ms/step - loss: 0.7991 - val_loss: 3.0132\n",
            "Epoch 15/100\n",
            "5162/5162 [==============================] - 99s 19ms/step - loss: 0.7161 - val_loss: 2.9918\n",
            "Epoch 16/100\n",
            "5162/5162 [==============================] - 99s 19ms/step - loss: 0.6658 - val_loss: 3.0218\n",
            "Epoch 17/100\n",
            "5162/5162 [==============================] - 99s 19ms/step - loss: 0.6035 - val_loss: 3.0218\n",
            "Epoch 18/100\n",
            "5162/5162 [==============================] - 100s 19ms/step - loss: 0.5580 - val_loss: 2.9853\n",
            "Epoch 19/100\n",
            "5162/5162 [==============================] - 100s 19ms/step - loss: 0.5100 - val_loss: 2.9618\n",
            "Epoch 20/100\n",
            "5162/5162 [==============================] - 99s 19ms/step - loss: 0.4612 - val_loss: 2.9990\n",
            "Epoch 21/100\n",
            "5162/5162 [==============================] - 99s 19ms/step - loss: 0.4492 - val_loss: 2.9796\n",
            "Epoch 22/100\n",
            "5162/5162 [==============================] - 99s 19ms/step - loss: 0.4132 - val_loss: 2.9018\n",
            "Epoch 23/100\n",
            "5162/5162 [==============================] - 100s 19ms/step - loss: 0.3831 - val_loss: 2.8324\n",
            "Epoch 24/100\n",
            "5162/5162 [==============================] - 100s 19ms/step - loss: 0.3549 - val_loss: 2.9286\n",
            "Epoch 25/100\n",
            "5162/5162 [==============================] - 99s 19ms/step - loss: 0.3334 - val_loss: 2.8496\n",
            "Epoch 26/100\n",
            "5162/5162 [==============================] - 100s 19ms/step - loss: 0.3136 - val_loss: 2.8823\n",
            "Epoch 27/100\n",
            "5162/5162 [==============================] - 100s 19ms/step - loss: 0.2990 - val_loss: 2.9288\n",
            "Epoch 28/100\n",
            "5162/5162 [==============================] - 100s 19ms/step - loss: 0.2867 - val_loss: 2.8425\n",
            " finished lm deep=True, mse: 2.75\n",
            "Epoch 1/100\n",
            "5162/5162 [==============================] - 106s 20ms/step - loss: 9.7270 - val_loss: 4.7892\n",
            "Epoch 2/100\n",
            "5162/5162 [==============================] - 105s 20ms/step - loss: 4.2163 - val_loss: 4.0663\n",
            "Epoch 3/100\n",
            "5162/5162 [==============================] - 105s 20ms/step - loss: 3.2933 - val_loss: 3.7951\n",
            "Epoch 4/100\n",
            "5162/5162 [==============================] - 104s 20ms/step - loss: 2.6836 - val_loss: 3.5734\n",
            "Epoch 5/100\n",
            "5162/5162 [==============================] - 104s 20ms/step - loss: 2.2018 - val_loss: 3.4242\n",
            "Epoch 6/100\n",
            "5162/5162 [==============================] - 102s 20ms/step - loss: 1.8669 - val_loss: 3.3749\n",
            "Epoch 7/100\n",
            "5162/5162 [==============================] - 102s 20ms/step - loss: 1.5868 - val_loss: 3.1985\n",
            "Epoch 8/100\n",
            "5162/5162 [==============================] - 103s 20ms/step - loss: 1.3554 - val_loss: 3.1594\n",
            "Epoch 9/100\n",
            "5162/5162 [==============================] - 102s 20ms/step - loss: 1.1945 - val_loss: 3.0939\n",
            "Epoch 10/100\n",
            "5162/5162 [==============================] - 102s 20ms/step - loss: 1.0504 - val_loss: 3.1580\n",
            "Epoch 11/100\n",
            "5162/5162 [==============================] - 102s 20ms/step - loss: 0.9241 - val_loss: 3.0407\n",
            "Epoch 12/100\n",
            "5162/5162 [==============================] - 102s 20ms/step - loss: 0.8372 - val_loss: 3.0507\n",
            "Epoch 13/100\n",
            "5162/5162 [==============================] - 102s 20ms/step - loss: 0.7613 - val_loss: 2.9702\n",
            "Epoch 14/100\n",
            "5162/5162 [==============================] - 102s 20ms/step - loss: 0.6794 - val_loss: 2.9812\n",
            "Epoch 15/100\n",
            "5162/5162 [==============================] - 102s 20ms/step - loss: 0.6162 - val_loss: 2.9090\n",
            "Epoch 16/100\n",
            "5162/5162 [==============================] - 103s 20ms/step - loss: 0.5595 - val_loss: 2.9182\n",
            "Epoch 17/100\n",
            "5162/5162 [==============================] - 103s 20ms/step - loss: 0.5130 - val_loss: 2.9870\n",
            "Epoch 18/100\n",
            "5162/5162 [==============================] - 103s 20ms/step - loss: 0.4619 - val_loss: 2.8694\n",
            "Epoch 19/100\n",
            "5162/5162 [==============================] - 103s 20ms/step - loss: 0.4317 - val_loss: 2.8672\n",
            "Epoch 20/100\n",
            "5162/5162 [==============================] - 103s 20ms/step - loss: 0.4005 - val_loss: 2.7892\n",
            "Epoch 21/100\n",
            "5162/5162 [==============================] - 103s 20ms/step - loss: 0.3848 - val_loss: 2.8531\n",
            "Epoch 22/100\n",
            "5162/5162 [==============================] - 108s 21ms/step - loss: 0.3451 - val_loss: 2.8208\n",
            "Epoch 23/100\n",
            "5162/5162 [==============================] - 107s 21ms/step - loss: 0.3267 - val_loss: 2.7700\n",
            "Epoch 24/100\n",
            "5162/5162 [==============================] - 106s 21ms/step - loss: 0.2989 - val_loss: 2.7562\n",
            "Epoch 25/100\n",
            "5162/5162 [==============================] - 106s 21ms/step - loss: 0.2955 - val_loss: 2.7568\n",
            "Epoch 26/100\n",
            "5162/5162 [==============================] - 105s 20ms/step - loss: 0.2667 - val_loss: 2.7939\n",
            "Epoch 27/100\n",
            "5162/5162 [==============================] - 105s 20ms/step - loss: 0.2547 - val_loss: 2.8190\n",
            "Epoch 28/100\n",
            "5162/5162 [==============================] - 106s 20ms/step - loss: 0.2426 - val_loss: 2.7284\n",
            "Epoch 29/100\n",
            "5162/5162 [==============================] - 105s 20ms/step - loss: 0.2342 - val_loss: 2.7641\n",
            "Epoch 30/100\n",
            "5162/5162 [==============================] - 105s 20ms/step - loss: 0.2152 - val_loss: 2.7241\n",
            "Epoch 31/100\n",
            "5162/5162 [==============================] - 106s 21ms/step - loss: 0.2081 - val_loss: 2.7497\n",
            "Epoch 32/100\n",
            "5162/5162 [==============================] - 109s 21ms/step - loss: 0.2035 - val_loss: 2.7278\n",
            "Epoch 33/100\n",
            "5162/5162 [==============================] - 106s 21ms/step - loss: 0.1903 - val_loss: 2.7201\n",
            "Epoch 34/100\n",
            "5162/5162 [==============================] - 106s 21ms/step - loss: 0.1817 - val_loss: 2.6954\n",
            "Epoch 35/100\n",
            "5162/5162 [==============================] - 106s 21ms/step - loss: 0.1737 - val_loss: 2.6839\n",
            "Epoch 36/100\n",
            "5162/5162 [==============================] - 106s 21ms/step - loss: 0.1793 - val_loss: 2.6792\n",
            "Epoch 37/100\n",
            "5162/5162 [==============================] - 107s 21ms/step - loss: 0.1603 - val_loss: 2.7257\n",
            "Epoch 38/100\n",
            "5162/5162 [==============================] - 106s 21ms/step - loss: 0.1606 - val_loss: 2.6793\n",
            "Epoch 39/100\n",
            "5162/5162 [==============================] - 110s 21ms/step - loss: 0.1522 - val_loss: 2.6930\n",
            "Epoch 40/100\n",
            "5162/5162 [==============================] - 110s 21ms/step - loss: 0.1512 - val_loss: 2.7218\n",
            "Epoch 41/100\n",
            "5162/5162 [==============================] - 109s 21ms/step - loss: 0.1405 - val_loss: 2.6888\n",
            " finished embed deep=True, mse: 2.60\n",
            "iteration 2, deep=True, mse change from mse_lm: -3.90%\n",
            "iteration 3\n",
            "Epoch 1/100\n",
            "5162/5162 [==============================] - 103s 20ms/step - loss: 9.8427 - val_loss: 4.8195\n",
            "Epoch 2/100\n",
            "5162/5162 [==============================] - 101s 20ms/step - loss: 4.2474 - val_loss: 4.2424\n",
            "Epoch 3/100\n",
            "5162/5162 [==============================] - 101s 20ms/step - loss: 3.3805 - val_loss: 4.0024\n",
            "Epoch 4/100\n",
            "5162/5162 [==============================] - 102s 20ms/step - loss: 2.7991 - val_loss: 3.7832\n",
            "Epoch 5/100\n",
            "5162/5162 [==============================] - 102s 20ms/step - loss: 2.2935 - val_loss: 3.6508\n",
            "Epoch 6/100\n",
            "5162/5162 [==============================] - 102s 20ms/step - loss: 1.9504 - val_loss: 3.5212\n",
            "Epoch 7/100\n",
            "5162/5162 [==============================] - 102s 20ms/step - loss: 1.6937 - val_loss: 3.5288\n",
            "Epoch 8/100\n",
            "5162/5162 [==============================] - 103s 20ms/step - loss: 1.4751 - val_loss: 3.4630\n",
            "Epoch 9/100\n",
            "5162/5162 [==============================] - 103s 20ms/step - loss: 1.2817 - val_loss: 3.4053\n",
            "Epoch 10/100\n",
            "5162/5162 [==============================] - 103s 20ms/step - loss: 1.1388 - val_loss: 3.2588\n",
            "Epoch 11/100\n",
            "5162/5162 [==============================] - 103s 20ms/step - loss: 0.9977 - val_loss: 3.3046\n",
            "Epoch 12/100\n",
            "5162/5162 [==============================] - 103s 20ms/step - loss: 0.8853 - val_loss: 3.1976\n",
            "Epoch 13/100\n",
            "5162/5162 [==============================] - 103s 20ms/step - loss: 0.8004 - val_loss: 3.2734\n",
            "Epoch 14/100\n",
            "5162/5162 [==============================] - 103s 20ms/step - loss: 0.7195 - val_loss: 3.2053\n",
            "Epoch 15/100\n",
            "5162/5162 [==============================] - 103s 20ms/step - loss: 0.6527 - val_loss: 3.1851\n",
            "Epoch 16/100\n",
            "5162/5162 [==============================] - 103s 20ms/step - loss: 0.5852 - val_loss: 3.0689\n",
            "Epoch 17/100\n",
            "5162/5162 [==============================] - 103s 20ms/step - loss: 0.5382 - val_loss: 3.0943\n",
            "Epoch 18/100\n",
            "5162/5162 [==============================] - 103s 20ms/step - loss: 0.4995 - val_loss: 3.0730\n",
            "Epoch 19/100\n",
            "5162/5162 [==============================] - 102s 20ms/step - loss: 0.4748 - val_loss: 3.0103\n",
            "Epoch 20/100\n",
            "5162/5162 [==============================] - 103s 20ms/step - loss: 0.4303 - val_loss: 2.9366\n",
            "Epoch 21/100\n",
            "5162/5162 [==============================] - 102s 20ms/step - loss: 0.4031 - val_loss: 2.9555\n",
            "Epoch 22/100\n",
            "5162/5162 [==============================] - 102s 20ms/step - loss: 0.3717 - val_loss: 2.9508\n",
            "Epoch 23/100\n",
            "5162/5162 [==============================] - 102s 20ms/step - loss: 0.3478 - val_loss: 2.9552\n",
            "Epoch 24/100\n",
            "5162/5162 [==============================] - 102s 20ms/step - loss: 0.3307 - val_loss: 2.9796\n",
            "Epoch 25/100\n",
            "5162/5162 [==============================] - 102s 20ms/step - loss: 0.3145 - val_loss: 2.9122\n",
            "Epoch 26/100\n",
            "5162/5162 [==============================] - 102s 20ms/step - loss: 0.2897 - val_loss: 2.9398\n",
            "Epoch 27/100\n",
            "5162/5162 [==============================] - 102s 20ms/step - loss: 0.2852 - val_loss: 2.9231\n",
            "Epoch 28/100\n",
            "5162/5162 [==============================] - 102s 20ms/step - loss: 0.2617 - val_loss: 2.9064\n",
            "Epoch 29/100\n",
            "5162/5162 [==============================] - 102s 20ms/step - loss: 0.2464 - val_loss: 2.9229\n",
            "Epoch 30/100\n",
            "5162/5162 [==============================] - 101s 20ms/step - loss: 0.2334 - val_loss: 2.8751\n",
            "Epoch 31/100\n",
            "5162/5162 [==============================] - 101s 20ms/step - loss: 0.2316 - val_loss: 2.8671\n",
            "Epoch 32/100\n",
            "5162/5162 [==============================] - 101s 20ms/step - loss: 0.2182 - val_loss: 2.8282\n",
            "Epoch 33/100\n",
            "5162/5162 [==============================] - 102s 20ms/step - loss: 0.2070 - val_loss: 2.8326\n",
            "Epoch 34/100\n",
            "5162/5162 [==============================] - 101s 20ms/step - loss: 0.2037 - val_loss: 2.8160\n",
            "Epoch 35/100\n",
            "5162/5162 [==============================] - 101s 20ms/step - loss: 0.1982 - val_loss: 2.8564\n",
            "Epoch 36/100\n",
            "5162/5162 [==============================] - 100s 19ms/step - loss: 0.1902 - val_loss: 2.8233\n",
            "Epoch 37/100\n",
            "5162/5162 [==============================] - 101s 19ms/step - loss: 0.1745 - val_loss: 2.8528\n",
            "Epoch 38/100\n",
            "5162/5162 [==============================] - 101s 19ms/step - loss: 0.1770 - val_loss: 2.8228\n",
            "Epoch 39/100\n",
            "5162/5162 [==============================] - 100s 19ms/step - loss: 0.1650 - val_loss: 2.8709\n",
            " finished ignore deep=True, mse: 2.82\n",
            "Epoch 1/100\n",
            "5162/5162 [==============================] - 112s 21ms/step - loss: 101.9382 - val_loss: 66.6377\n",
            "Epoch 2/100\n",
            "5162/5162 [==============================] - 109s 21ms/step - loss: 64.7911 - val_loss: 65.0771\n",
            "Epoch 3/100\n",
            "5162/5162 [==============================] - 110s 21ms/step - loss: 61.1440 - val_loss: 63.7851\n",
            "Epoch 4/100\n",
            "5162/5162 [==============================] - 110s 21ms/step - loss: 57.7923 - val_loss: 63.1660\n",
            "Epoch 5/100\n",
            "5162/5162 [==============================] - 110s 21ms/step - loss: 54.9494 - val_loss: 63.7359\n",
            "Epoch 6/100\n",
            "5162/5162 [==============================] - 110s 21ms/step - loss: 52.2841 - val_loss: 64.2703\n",
            "Epoch 7/100\n",
            "5162/5162 [==============================] - 110s 21ms/step - loss: 49.8250 - val_loss: 64.8419\n",
            "Epoch 8/100\n",
            "5162/5162 [==============================] - 110s 21ms/step - loss: 47.7860 - val_loss: 67.7528\n",
            "Epoch 9/100\n",
            "5162/5162 [==============================] - 110s 21ms/step - loss: 45.5718 - val_loss: 68.7608\n",
            "Epoch 10/100\n",
            "5162/5162 [==============================] - 110s 21ms/step - loss: 43.5619 - val_loss: 72.1250\n",
            "Epoch 11/100\n",
            "5162/5162 [==============================] - 111s 21ms/step - loss: 41.8640 - val_loss: 74.3419\n",
            " sig2e: 0.98, sig2b: 0.02\n",
            "Epoch 12/100\n",
            "5162/5162 [==============================] - 110s 21ms/step - loss: 40.0976 - val_loss: 76.8319\n",
            "Epoch 13/100\n",
            "5162/5162 [==============================] - 110s 21ms/step - loss: 38.3911 - val_loss: 80.2948\n",
            "Epoch 14/100\n",
            "5162/5162 [==============================] - 110s 21ms/step - loss: 36.9769 - val_loss: 85.1501\n",
            "Epoch 15/100\n",
            "5162/5162 [==============================] - 110s 21ms/step - loss: 35.4749 - val_loss: 87.6232\n",
            "Epoch 16/100\n",
            "5162/5162 [==============================] - 110s 21ms/step - loss: 34.0941 - val_loss: 96.5808\n",
            "Epoch 17/100\n",
            "5162/5162 [==============================] - 110s 21ms/step - loss: 32.6683 - val_loss: 101.3792\n",
            "Epoch 18/100\n",
            "5162/5162 [==============================] - 110s 21ms/step - loss: 31.5858 - val_loss: 103.7529\n",
            "Epoch 19/100\n",
            "5162/5162 [==============================] - 110s 21ms/step - loss: 30.3375 - val_loss: 105.8538\n",
            "Epoch 20/100\n",
            "5162/5162 [==============================] - 110s 21ms/step - loss: 29.0363 - val_loss: 120.6165\n",
            "Epoch 21/100\n",
            "5162/5162 [==============================] - 110s 21ms/step - loss: 27.6581 - val_loss: 117.6849\n",
            " sig2e: 0.40, sig2b: 0.03\n",
            "Epoch 22/100\n",
            "5162/5162 [==============================] - 111s 22ms/step - loss: 27.2159 - val_loss: 126.1952\n",
            "Epoch 23/100\n",
            "5162/5162 [==============================] - 111s 21ms/step - loss: 26.3607 - val_loss: 133.5261\n",
            "Epoch 24/100\n",
            "5162/5162 [==============================] - 110s 21ms/step - loss: 25.1413 - val_loss: 128.3375\n",
            "Epoch 25/100\n",
            "5162/5162 [==============================] - 111s 22ms/step - loss: 24.6247 - val_loss: 150.2596\n",
            "Epoch 26/100\n",
            "5162/5162 [==============================] - 111s 22ms/step - loss: 23.2413 - val_loss: 150.3761\n",
            "Epoch 27/100\n",
            "5162/5162 [==============================] - 111s 21ms/step - loss: 22.8621 - val_loss: 156.1904\n",
            "Epoch 28/100\n",
            "5162/5162 [==============================] - 109s 21ms/step - loss: 22.0322 - val_loss: 160.4434\n",
            "Epoch 29/100\n",
            "5162/5162 [==============================] - 108s 21ms/step - loss: 21.2323 - val_loss: 166.5188\n",
            "Epoch 30/100\n",
            "5162/5162 [==============================] - 110s 21ms/step - loss: 20.5439 - val_loss: 175.1594\n",
            "Epoch 31/100\n",
            "5162/5162 [==============================] - 109s 21ms/step - loss: 19.6895 - val_loss: 177.4195\n",
            " sig2e: 0.23, sig2b: 0.02\n",
            "Epoch 32/100\n",
            "5162/5162 [==============================] - 109s 21ms/step - loss: 19.0520 - val_loss: 179.9986\n",
            "Epoch 33/100\n",
            "5162/5162 [==============================] - 109s 21ms/step - loss: 18.6764 - val_loss: 208.3065\n",
            "Epoch 34/100\n",
            "5162/5162 [==============================] - 108s 21ms/step - loss: 18.1600 - val_loss: 195.9129\n",
            "Epoch 35/100\n",
            "5162/5162 [==============================] - 108s 21ms/step - loss: 17.6717 - val_loss: 206.6080\n",
            "Epoch 36/100\n",
            "5162/5162 [==============================] - 109s 21ms/step - loss: 16.9767 - val_loss: 236.1583\n",
            "Epoch 37/100\n",
            "5162/5162 [==============================] - 109s 21ms/step - loss: 16.7543 - val_loss: 209.6798\n",
            "Epoch 38/100\n",
            "5162/5162 [==============================] - 108s 21ms/step - loss: 15.9930 - val_loss: 231.4317\n",
            "Epoch 39/100\n",
            "5162/5162 [==============================] - 108s 21ms/step - loss: 15.1498 - val_loss: 227.9968\n",
            "Epoch 40/100\n",
            "5162/5162 [==============================] - 108s 21ms/step - loss: 14.6773 - val_loss: 227.9470\n",
            "Epoch 41/100\n",
            "5162/5162 [==============================] - 108s 21ms/step - loss: 14.5657 - val_loss: 226.8090\n",
            " sig2e: 0.17, sig2b: 0.02\n",
            "Epoch 42/100\n",
            "5162/5162 [==============================] - 109s 21ms/step - loss: 14.2296 - val_loss: 246.1111\n",
            "Epoch 43/100\n",
            "5162/5162 [==============================] - 108s 21ms/step - loss: 13.6394 - val_loss: 270.7794\n",
            "Epoch 44/100\n",
            "5162/5162 [==============================] - 108s 21ms/step - loss: 13.7014 - val_loss: 251.3174\n",
            "Epoch 45/100\n",
            "5162/5162 [==============================] - 108s 21ms/step - loss: 12.8061 - val_loss: 262.5042\n",
            "Epoch 46/100\n",
            "5162/5162 [==============================] - 109s 21ms/step - loss: 12.8130 - val_loss: 303.8798\n",
            "Epoch 47/100\n",
            "5162/5162 [==============================] - 109s 21ms/step - loss: 12.1490 - val_loss: 285.3700\n",
            "Epoch 48/100\n",
            "5162/5162 [==============================] - 108s 21ms/step - loss: 11.8485 - val_loss: 300.2957\n",
            "Epoch 49/100\n",
            "5162/5162 [==============================] - 108s 21ms/step - loss: 11.4731 - val_loss: 297.7721\n",
            "Epoch 50/100\n",
            "5162/5162 [==============================] - 109s 21ms/step - loss: 11.2862 - val_loss: 294.8036\n",
            "Epoch 51/100\n",
            "5162/5162 [==============================] - 108s 21ms/step - loss: 11.3116 - val_loss: 340.1920\n",
            " sig2e: 0.12, sig2b: 0.00\n",
            "Epoch 52/100\n",
            "5162/5162 [==============================] - 108s 21ms/step - loss: 10.7040 - val_loss: 333.3303\n",
            "Epoch 53/100\n",
            "5162/5162 [==============================] - 108s 21ms/step - loss: 10.6893 - val_loss: 355.5619\n",
            "Epoch 54/100\n",
            "5162/5162 [==============================] - 109s 21ms/step - loss: 10.8816 - val_loss: 328.7454\n",
            "Epoch 55/100\n",
            "5162/5162 [==============================] - 110s 21ms/step - loss: 9.7916 - val_loss: 290.5800\n",
            "Epoch 56/100\n",
            "5162/5162 [==============================] - 111s 21ms/step - loss: 9.7740 - val_loss: 326.2184\n",
            "Epoch 57/100\n",
            "5162/5162 [==============================] - 111s 21ms/step - loss: 9.4823 - val_loss: 308.4022\n",
            "Epoch 58/100\n",
            "5162/5162 [==============================] - 111s 21ms/step - loss: 9.0486 - val_loss: 331.7211\n",
            "Epoch 59/100\n",
            "5162/5162 [==============================] - 111s 22ms/step - loss: 8.9756 - val_loss: 373.8319\n",
            "Epoch 60/100\n",
            "5162/5162 [==============================] - 111s 21ms/step - loss: 9.3844 - val_loss: 373.5224\n",
            "Epoch 61/100\n",
            "5162/5162 [==============================] - 111s 22ms/step - loss: 8.3580 - val_loss: 365.7317\n",
            " sig2e: 0.10, sig2b: 0.01\n",
            "Epoch 62/100\n",
            "5162/5162 [==============================] - 111s 22ms/step - loss: 7.7501 - val_loss: 363.4725\n",
            "Epoch 63/100\n",
            "5162/5162 [==============================] - 112s 22ms/step - loss: 9.0555 - val_loss: 417.9081\n",
            "Epoch 64/100\n",
            "5162/5162 [==============================] - 111s 22ms/step - loss: 7.8488 - val_loss: 469.3450\n",
            "Epoch 65/100\n",
            "5162/5162 [==============================] - 111s 22ms/step - loss: 7.3176 - val_loss: 369.1657\n",
            "Epoch 66/100\n",
            "5162/5162 [==============================] - 111s 22ms/step - loss: 7.4776 - val_loss: 347.4290\n",
            "Epoch 67/100\n",
            "5162/5162 [==============================] - 111s 22ms/step - loss: 7.4763 - val_loss: 419.8053\n",
            "Epoch 68/100\n",
            "5162/5162 [==============================] - 111s 22ms/step - loss: 6.6364 - val_loss: 425.9172\n",
            "Epoch 69/100\n",
            "5162/5162 [==============================] - 112s 22ms/step - loss: 7.0545 - val_loss: 409.1089\n",
            "Epoch 70/100\n",
            "5162/5162 [==============================] - 111s 22ms/step - loss: 6.7059 - val_loss: 342.6552\n",
            " finished lmm deep=True, mse: 2.66\n",
            "Epoch 1/100\n",
            "5162/5162 [==============================] - 108s 21ms/step - loss: 11.9232 - val_loss: 5.9114\n",
            "Epoch 2/100\n",
            "5162/5162 [==============================] - 106s 20ms/step - loss: 4.9089 - val_loss: 4.5060\n",
            "Epoch 3/100\n",
            "5162/5162 [==============================] - 106s 21ms/step - loss: 3.7166 - val_loss: 4.0786\n",
            "Epoch 4/100\n",
            "5162/5162 [==============================] - 106s 21ms/step - loss: 2.9813 - val_loss: 3.9600\n",
            "Epoch 5/100\n",
            "5162/5162 [==============================] - 107s 21ms/step - loss: 2.4535 - val_loss: 3.6526\n",
            "Epoch 6/100\n",
            "5162/5162 [==============================] - 106s 21ms/step - loss: 2.0153 - val_loss: 3.4720\n",
            "Epoch 7/100\n",
            "5162/5162 [==============================] - 107s 21ms/step - loss: 1.6914 - val_loss: 3.3980\n",
            "Epoch 8/100\n",
            "5162/5162 [==============================] - 107s 21ms/step - loss: 1.4636 - val_loss: 3.3781\n",
            "Epoch 9/100\n",
            "5162/5162 [==============================] - 107s 21ms/step - loss: 1.2619 - val_loss: 3.2440\n",
            "Epoch 10/100\n",
            "5162/5162 [==============================] - 107s 21ms/step - loss: 1.0953 - val_loss: 3.2208\n",
            "Epoch 11/100\n",
            "5162/5162 [==============================] - 106s 21ms/step - loss: 0.9786 - val_loss: 3.1859\n",
            "Epoch 12/100\n",
            "5162/5162 [==============================] - 106s 21ms/step - loss: 0.8577 - val_loss: 3.1597\n",
            "Epoch 13/100\n",
            "5162/5162 [==============================] - 105s 20ms/step - loss: 0.7754 - val_loss: 3.0900\n",
            "Epoch 14/100\n",
            "5162/5162 [==============================] - 105s 20ms/step - loss: 0.6914 - val_loss: 3.1216\n",
            "Epoch 15/100\n",
            "5162/5162 [==============================] - 106s 21ms/step - loss: 0.6215 - val_loss: 3.0087\n",
            "Epoch 16/100\n",
            "5162/5162 [==============================] - 106s 20ms/step - loss: 0.5586 - val_loss: 2.9581\n",
            "Epoch 17/100\n",
            "5162/5162 [==============================] - 106s 21ms/step - loss: 0.5107 - val_loss: 2.9772\n",
            "Epoch 18/100\n",
            "5162/5162 [==============================] - 106s 21ms/step - loss: 0.4727 - val_loss: 3.0152\n",
            "Epoch 19/100\n",
            "5162/5162 [==============================] - 106s 20ms/step - loss: 0.4331 - val_loss: 2.8906\n",
            "Epoch 20/100\n",
            "5162/5162 [==============================] - 106s 20ms/step - loss: 0.3935 - val_loss: 2.8850\n",
            "Epoch 21/100\n",
            "5162/5162 [==============================] - 106s 21ms/step - loss: 0.3721 - val_loss: 2.8775\n",
            "Epoch 22/100\n",
            "5162/5162 [==============================] - 106s 20ms/step - loss: 0.3403 - val_loss: 2.8495\n",
            "Epoch 23/100\n",
            "5162/5162 [==============================] - 106s 21ms/step - loss: 0.3267 - val_loss: 2.8644\n",
            "Epoch 24/100\n",
            "5162/5162 [==============================] - 106s 20ms/step - loss: 0.3077 - val_loss: 2.8331\n",
            "Epoch 25/100\n",
            "5162/5162 [==============================] - 106s 21ms/step - loss: 0.2817 - val_loss: 2.8215\n",
            "Epoch 26/100\n",
            "5162/5162 [==============================] - 107s 21ms/step - loss: 0.2749 - val_loss: 2.8611\n",
            "Epoch 27/100\n",
            "5162/5162 [==============================] - 106s 21ms/step - loss: 0.2555 - val_loss: 2.8346\n",
            "Epoch 28/100\n",
            "5162/5162 [==============================] - 106s 20ms/step - loss: 0.2573 - val_loss: 2.8903\n",
            "Epoch 29/100\n",
            "5162/5162 [==============================] - 106s 21ms/step - loss: 0.2293 - val_loss: 2.8294\n",
            "Epoch 30/100\n",
            "5162/5162 [==============================] - 106s 21ms/step - loss: 0.2294 - val_loss: 2.8844\n",
            " finished lm deep=True, mse: 2.78\n",
            "Epoch 1/100\n",
            "5162/5162 [==============================] - 112s 21ms/step - loss: 9.5151 - val_loss: 4.6505\n",
            "Epoch 2/100\n",
            "5162/5162 [==============================] - 108s 21ms/step - loss: 4.1164 - val_loss: 4.2969\n",
            "Epoch 3/100\n",
            "5162/5162 [==============================] - 108s 21ms/step - loss: 3.2887 - val_loss: 3.8957\n",
            "Epoch 4/100\n",
            "5162/5162 [==============================] - 109s 21ms/step - loss: 2.7016 - val_loss: 3.8127\n",
            "Epoch 5/100\n",
            "5162/5162 [==============================] - 108s 21ms/step - loss: 2.2235 - val_loss: 3.5313\n",
            "Epoch 6/100\n",
            "5162/5162 [==============================] - 108s 21ms/step - loss: 1.8632 - val_loss: 3.4114\n",
            "Epoch 7/100\n",
            "5162/5162 [==============================] - 108s 21ms/step - loss: 1.5995 - val_loss: 3.3285\n",
            "Epoch 8/100\n",
            "5162/5162 [==============================] - 108s 21ms/step - loss: 1.4008 - val_loss: 3.2370\n",
            "Epoch 9/100\n",
            "5162/5162 [==============================] - 108s 21ms/step - loss: 1.2046 - val_loss: 3.1746\n",
            "Epoch 10/100\n",
            "5162/5162 [==============================] - 108s 21ms/step - loss: 1.0650 - val_loss: 3.2016\n",
            "Epoch 11/100\n",
            "5162/5162 [==============================] - 107s 21ms/step - loss: 0.9420 - val_loss: 3.0885\n",
            "Epoch 12/100\n",
            "5162/5162 [==============================] - 107s 21ms/step - loss: 0.8329 - val_loss: 3.1021\n",
            "Epoch 13/100\n",
            "5162/5162 [==============================] - 108s 21ms/step - loss: 0.7539 - val_loss: 3.0855\n",
            "Epoch 14/100\n",
            "5162/5162 [==============================] - 108s 21ms/step - loss: 0.6801 - val_loss: 3.0939\n",
            "Epoch 15/100\n",
            "5162/5162 [==============================] - 108s 21ms/step - loss: 0.6051 - val_loss: 3.0138\n",
            "Epoch 16/100\n",
            "5162/5162 [==============================] - 107s 21ms/step - loss: 0.5586 - val_loss: 3.0561\n",
            "Epoch 17/100\n",
            "5162/5162 [==============================] - 108s 21ms/step - loss: 0.5227 - val_loss: 3.0110\n",
            "Epoch 18/100\n",
            "5162/5162 [==============================] - 107s 21ms/step - loss: 0.4754 - val_loss: 2.9699\n",
            "Epoch 19/100\n",
            "5162/5162 [==============================] - 108s 21ms/step - loss: 0.4372 - val_loss: 2.9619\n",
            "Epoch 20/100\n",
            "5162/5162 [==============================] - 108s 21ms/step - loss: 0.4083 - val_loss: 2.9261\n",
            "Epoch 21/100\n",
            "5162/5162 [==============================] - 108s 21ms/step - loss: 0.3772 - val_loss: 2.9039\n",
            "Epoch 22/100\n",
            "5162/5162 [==============================] - 108s 21ms/step - loss: 0.3494 - val_loss: 2.8519\n",
            "Epoch 23/100\n",
            "5162/5162 [==============================] - 107s 21ms/step - loss: 0.3348 - val_loss: 2.8597\n",
            "Epoch 24/100\n",
            "5162/5162 [==============================] - 107s 21ms/step - loss: 0.3086 - val_loss: 2.8355\n",
            "Epoch 25/100\n",
            "5162/5162 [==============================] - 108s 21ms/step - loss: 0.2903 - val_loss: 2.8359\n",
            "Epoch 26/100\n",
            "5162/5162 [==============================] - 107s 21ms/step - loss: 0.2833 - val_loss: 2.8302\n",
            "Epoch 27/100\n",
            "5162/5162 [==============================] - 108s 21ms/step - loss: 0.2635 - val_loss: 2.7707\n",
            "Epoch 28/100\n",
            "5162/5162 [==============================] - 108s 21ms/step - loss: 0.2506 - val_loss: 2.7942\n",
            "Epoch 29/100\n",
            "5162/5162 [==============================] - 108s 21ms/step - loss: 0.2422 - val_loss: 2.7635\n",
            "Epoch 30/100\n",
            "5162/5162 [==============================] - 108s 21ms/step - loss: 0.2302 - val_loss: 2.8211\n",
            "Epoch 31/100\n",
            "5162/5162 [==============================] - 108s 21ms/step - loss: 0.2186 - val_loss: 2.8120\n",
            "Epoch 32/100\n",
            "5162/5162 [==============================] - 108s 21ms/step - loss: 0.2089 - val_loss: 2.7894\n",
            "Epoch 33/100\n",
            "5162/5162 [==============================] - 108s 21ms/step - loss: 0.2027 - val_loss: 2.7559\n",
            "Epoch 34/100\n",
            "5162/5162 [==============================] - 108s 21ms/step - loss: 0.1913 - val_loss: 2.8104\n",
            "Epoch 35/100\n",
            "5162/5162 [==============================] - 108s 21ms/step - loss: 0.1855 - val_loss: 2.8119\n",
            "Epoch 36/100\n",
            "5162/5162 [==============================] - 108s 21ms/step - loss: 0.1840 - val_loss: 2.7770\n",
            "Epoch 37/100\n",
            "5162/5162 [==============================] - 108s 21ms/step - loss: 0.1745 - val_loss: 2.7980\n",
            "Epoch 38/100\n",
            "5162/5162 [==============================] - 107s 21ms/step - loss: 0.1657 - val_loss: 2.7595\n",
            " finished embed deep=True, mse: 2.68\n",
            "iteration 3, deep=True, mse change from mse_lm: -4.43%\n",
            "iteration 4\n",
            "Epoch 1/100\n",
            "5162/5162 [==============================] - 104s 20ms/step - loss: 10.0788 - val_loss: 4.7812\n",
            "Epoch 2/100\n",
            "  39/5162 [..............................] - ETA: 1:39 - loss: 4.1112"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-49279d720fa5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'iteration %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0miterate_reg_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-25-49279d720fa5>\u001b[0m in \u001b[0;36miterate_reg_types\u001b[0;34m(X_train, X_test, y_train, y_test, deep)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0miterate_reg_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mmse_ig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreg_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_cats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' finished ignore deep=%s, mse: %.2f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse_ig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmse_lmm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreg_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_cats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lmm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-af0ae35aa298>\u001b[0m in \u001b[0;36mreg_nn\u001b[0;34m(X_train, X_test, y_train, y_test, n_cats, batch, epochs, patience, reg_type, deep)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreg_nn_lmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_cats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mreg_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreg_nn_ignore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_cats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreg_nn_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_cats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-af0ae35aa298>\u001b[0m in \u001b[0;36mreg_nn_ignore\u001b[0;34m(X_train, X_test, y_train, y_test, n_cats, batch_size, epochs, patience, deep)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpatience\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     history = model.fit(X_train[x_cols], y_train, batch_size=batch_size, epochs=epochs,\n\u001b[0;32m----> 7\u001b[0;31m                         validation_split=0.1, callbacks=callbacks, verbose=1)\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "FPSjULlZWgZp",
        "outputId": "a750ed18-05a8-4357-f2fb-99e24105f506"
      },
      "source": [
        "res"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>experiment</th>\n",
              "      <th>exp_type</th>\n",
              "      <th>deep</th>\n",
              "      <th>mse</th>\n",
              "      <th>sigma_e_est</th>\n",
              "      <th>sigma_b_est</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>ohe</td>\n",
              "      <td>True</td>\n",
              "      <td>2.761191</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>lmm</td>\n",
              "      <td>True</td>\n",
              "      <td>2.680337</td>\n",
              "      <td>0.092918</td>\n",
              "      <td>0.007628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>ignore</td>\n",
              "      <td>True</td>\n",
              "      <td>2.779226</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>embed</td>\n",
              "      <td>True</td>\n",
              "      <td>2.735665</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>ohe</td>\n",
              "      <td>True</td>\n",
              "      <td>2.775354</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>lmm</td>\n",
              "      <td>True</td>\n",
              "      <td>2.674060</td>\n",
              "      <td>0.108524</td>\n",
              "      <td>0.024346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>ignore</td>\n",
              "      <td>True</td>\n",
              "      <td>2.745295</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1</td>\n",
              "      <td>embed</td>\n",
              "      <td>True</td>\n",
              "      <td>2.907608</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2</td>\n",
              "      <td>ohe</td>\n",
              "      <td>True</td>\n",
              "      <td>2.753927</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2</td>\n",
              "      <td>lmm</td>\n",
              "      <td>True</td>\n",
              "      <td>2.646433</td>\n",
              "      <td>0.091408</td>\n",
              "      <td>0.007817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2</td>\n",
              "      <td>ignore</td>\n",
              "      <td>True</td>\n",
              "      <td>2.625448</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2</td>\n",
              "      <td>embed</td>\n",
              "      <td>True</td>\n",
              "      <td>2.597890</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>3</td>\n",
              "      <td>ohe</td>\n",
              "      <td>True</td>\n",
              "      <td>2.779261</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>3</td>\n",
              "      <td>lmm</td>\n",
              "      <td>True</td>\n",
              "      <td>2.656111</td>\n",
              "      <td>0.106242</td>\n",
              "      <td>0.011266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>3</td>\n",
              "      <td>ignore</td>\n",
              "      <td>True</td>\n",
              "      <td>2.815019</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>3</td>\n",
              "      <td>embed</td>\n",
              "      <td>True</td>\n",
              "      <td>2.679253</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   experiment exp_type  deep       mse  sigma_e_est  sigma_b_est\n",
              "0           0      ohe  True  2.761191          NaN          NaN\n",
              "1           0      lmm  True  2.680337     0.092918     0.007628\n",
              "2           0   ignore  True  2.779226          NaN          NaN\n",
              "3           0    embed  True  2.735665          NaN          NaN\n",
              "4           1      ohe  True  2.775354          NaN          NaN\n",
              "5           1      lmm  True  2.674060     0.108524     0.024346\n",
              "6           1   ignore  True  2.745295          NaN          NaN\n",
              "7           1    embed  True  2.907608          NaN          NaN\n",
              "8           2      ohe  True  2.753927          NaN          NaN\n",
              "9           2      lmm  True  2.646433     0.091408     0.007817\n",
              "10          2   ignore  True  2.625448          NaN          NaN\n",
              "11          2    embed  True  2.597890          NaN          NaN\n",
              "12          3      ohe  True  2.779261          NaN          NaN\n",
              "13          3      lmm  True  2.656111     0.106242     0.011266\n",
              "14          3   ignore  True  2.815019          NaN          NaN\n",
              "15          3    embed  True  2.679253          NaN          NaN"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "res.to_csv('../results/drugs.csv')"
      ]
    }
  ]
}